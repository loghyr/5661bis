<!-- Copyright (C) The IETF Trust (2011) -->
<!-- Copyright (C) The Internet Society (2011) -->

<section anchor="sec:core_infrastructure" title="Core Infrastructure">
  <section anchor="ss:core_infrastructure:I" title="Introduction">
    <t>
      NFSv4.1 relies on core infrastructure common to nearly every
      operation.  This core infrastructure is described in the remainder of
      this section.
    </t>
  </section>

  <section anchor="ss:core_infrastructure:RaX" title="RPC and XDR">
    <t>
      The NFSv4.1 protocol is a Remote Procedure Call (RPC) application
      that uses RPC version 2 and the corresponding eXternal Data
      Representation (XDR) as defined in
      <xref target="RFC5531" /> and <xref target="RFC4506" />.
    </t>

    <section toc='exclude' anchor="ss:core_infrastructure:RS" title="RPC-Based Security">
      <t>
        Previous NFS versions have been thought of as having a host-based
        authentication model, where the NFS server authenticates the NFS
        client, and trusts the client to authenticate all users.  Actually,
        NFS has always depended on RPC for authentication.  One of the first
        forms of RPC authentication, AUTH_SYS, had no strong authentication
        and required a host-based authentication approach.  NFSv4.1 also
        depends on RPC for basic security services and mandates RPC support
        for a user-based authentication model.  The user-based authentication
        model has user principals authenticated by a server, and in turn the
        server authenticated by user principals.  RPC provides some basic
        security services that are used by NFSv4.1.
      </t>

      <section toc='exclude' anchor="ss:core_infrastructure:RSF" title="RPC Security Flavors">
        <t>
          As described in Section 7.2 ("Authentication") of <xref target="RFC5531" />, RPC security
          is encapsulated in the RPC header, via a security or authentication
          flavor, and information specific to the specified security flavor.
          Every RPC header conveys information used to identify and
          authenticate a client and server.  As discussed in <xref target="ss:core_infrastructure:RaSS" />,
          some security flavors provide additional security services.
        </t>

        <t>
          NFSv4.1 clients and servers MUST implement RPCSEC_GSS.  (This
          requirement to implement is not a requirement to use.)  Other
          flavors, such as AUTH_NONE and AUTH_SYS, MAY be implemented as well.
        </t>

        <section toc='exclude' anchor="ss:core_infrastructure:RaSS" title="RPCSEC_GSS and Security Services">
          <t>
            RPCSEC_GSS <xref target="RFC2203" /> uses the functionality of GSS-API <xref target="RFC2743" />.  This allows
            for the use of various security mechanisms by the RPC layer without
            the additional implementation overhead of adding RPC security
            flavors.
          </t>

          <section toc='exclude' anchor="ss:core_infrastructure:IAIP" title="Identification, Authentication, Integrity, Privacy">
            <t>
              Via the GSS-API, RPCSEC_GSS can be used to identify and authenticate
              users on clients to servers, and servers to users.  It can also
              perform integrity checking on the entire RPC message, including the
              RPC header, and on the arguments or results.  Finally, privacy,
              usually via encryption, is a service available with RPCSEC_GSS.
              Privacy is performed on the arguments and results.  Note that if
              privacy is selected, integrity, authentication, and identification
              are enabled.  If privacy is not selected, but integrity is selected,
              authentication and identification are enabled.  If integrity and
              privacy are not selected, but authentication is enabled,
              identification is enabled.  RPCSEC_GSS does not provide
              identification as a separate service.
            </t>

            <t>
              Although GSS-API has an authentication service distinct from its
              privacy and integrity services, GSS-API's authentication service is
              not used for RPCSEC_GSS's authentication service.  Instead, each RPC
              request and response header is integrity protected with the GSS-API
              integrity service, and this allows RPCSEC_GSS to offer per-RPC
              authentication and identity.  See <xref target="RFC2203" /> for more information.
            </t>

            <t>
              NFSv4.1 client and servers MUST support RPCSEC_GSS's integrity and
              authentication service.  NFSv4.1 servers MUST support RPCSEC_GSS's
              privacy service.  NFSv4.1 clients SHOULD support RPCSEC_GSS's privacy
              service.
            </t>
          </section>

          <section toc='exclude' anchor="ss:core_infrastructure:SMfN" title="Security Mechanisms for NFSv4.1">
            <t>
              RPCSEC_GSS, via GSS-API, normalizes access to mechanisms that provide
              security services.  Therefore, NFSv4.1 clients and servers MUST
              support the Kerberos V5 security mechanism.
            </t>

            <t>
              The use of RPCSEC_GSS requires selection of mechanism, quality of
              protection (QOP), and service (authentication, integrity, privacy).
              For the mandated security mechanisms, NFSv4.1 specifies that a QOP of
              zero is used, leaving it up to the mechanism or the mechanism's
              configuration to map QOP zero to an appropriate level of protection.
              Each mandated mechanism specifies a minimum set of cryptographic
              algorithms for implementing integrity and privacy.  NFSv4.1 clients
              and servers MUST be implemented on operating environments that comply
              with the REQUIRED cryptographic algorithms of each REQUIRED
              mechanism.
            </t>

            <section toc='exclude' anchor="ss:core_infrastructure:KV" title="Kerberos V5">
              <t>
                The Kerberos V5 GSS-API mechanism as described in <xref target="RFC4121" /> MUST be
                implemented with the RPCSEC_GSS services as specified in the
                following table:
              </t>

              <figure>
                <artwork>
   column descriptions:
   1 == number of pseudo flavor
   2 == name of pseudo flavor
   3 == mechanism's OID
   4 == RPCSEC_GSS service
   5 == NFSv4.1 clients MUST support
   6 == NFSv4.1 servers MUST support

   1      2        3                    4                     5   6
   ------------------------------------------------------------------
   390003 krb5     1.2.840.113554.1.2.2 rpc_gss_svc_none      yes yes
   390004 krb5i    1.2.840.113554.1.2.2 rpc_gss_svc_integrity yes yes
   390005 krb5p    1.2.840.113554.1.2.2 rpc_gss_svc_privacy    no yes
                </artwork>
              </figure>

              <t>
                Note that the number and name of the pseudo flavor are presented here
                as a mapping aid to the implementor.  Because the NFSv4.1 protocol
                includes a method to negotiate security and it understands the GSS-API
                mechanism, the pseudo flavor is not needed.  The pseudo flavor is
                needed for the NFSv3 since the security negotiation is done via the
                MOUNT protocol as described in <xref target="RFC2623" />.
              </t>

              <t>
                At the time NFSv4.1 was specified, the Advanced Encryption Standard
                (AES) with HMAC-SHA1 was a REQUIRED algorithm set for Kerberos V5.
                In contrast, when NFSv4.0 was specified, weaker algorithm sets were
                REQUIRED for Kerberos V5, and were REQUIRED in the NFSv4.0
                specification, because the Kerberos V5 specification at the time did
                not specify stronger algorithms.  The NFSv4.1 specification does not
                specify REQUIRED algorithms for Kerberos V5, and instead, the
                implementor is expected to track the evolution of the Kerberos V5
                standard if and when stronger algorithms are specified.
              </t>

              <section toc='exclude' anchor="ss:core_infrastructure:SCfCAiKV" title="Security Considerations for Cryptographic Algorithms in Kerberos V5">
                <t>
                  When deploying NFSv4.1, the strength of the security achieved depends
                  on the existing Kerberos V5 infrastructure.  The algorithms of
                  Kerberos V5 are not directly exposed to or selectable by the client
                  or server, so there is some due diligence required by the user of
                  NFSv4.1 to ensure that security is acceptable where needed.
                </t>
              </section>
            </section>
          </section>

          <section toc='exclude' anchor="ss:core_infrastructure:GSP" title="GSS Server Principal">
            <t>
              Regardless of what security mechanism under RPCSEC_GSS is being used,
              the NFS server MUST identify itself in GSS-API via a
              GSS_C_NT_HOSTBASED_SERVICE name type.  GSS_C_NT_HOSTBASED_SERVICE
              names are of the form:
            </t>

            <figure>
              <artwork>
        service@hostname
              </artwork>
            </figure>

            <t>
              For NFS, the "service" element is
            </t>

            <figure>
              <artwork>
        nfs
              </artwork>
            </figure>

            <t>
              Implementations of security mechanisms will convert nfs@hostname to
              various different forms.  For Kerberos V5, the following form is
              RECOMMENDED:
            </t>

            <figure>
              <artwork>
        nfs/hostname
              </artwork>
            </figure>
          </section>
        </section>
      </section>
    </section>
  </section>

  <section anchor="ss:core_infrastructure:CaC" title="COMPOUND and CB_COMPOUND">
    <t>
      A significant departure from the versions of the NFS protocol before
      NFSv4 is the introduction of the COMPOUND procedure.  For the NFSv4
      protocol, in all minor versions, there are exactly two RPC
      procedures, NULL and COMPOUND.  The COMPOUND procedure is defined as
      a series of individual operations and these operations perform the
      sorts of functions performed by traditional NFS procedures.
    </t>

    <t>
      The operations combined within a COMPOUND request are evaluated in
      order by the server, without any atomicity guarantees.  A limited set
      of facilities exist to pass results from one operation to another.
      Once an operation returns a failing result, the evaluation ends and
      the results of all evaluated operations are returned to the client.
    </t>

    <t>
      With the use of the COMPOUND procedure, the client is able to build
      simple or complex requests.  These COMPOUND requests allow for a
      reduction in the number of RPCs needed for logical file system
      operations.  For example, multi-component look up requests can be
      constructed by combining multiple LOOKUP operations.  Those can be
      further combined with operations such as GETATTR, READDIR, or OPEN
      plus READ to do more complicated sets of operation without incurring
      additional latency.
    </t>

    <t>
      NFSv4.1 also contains a considerable set of callback operations in
      which the server makes an RPC directed at the client.  Callback RPCs
      have a similar structure to that of the normal server requests.  In
      all minor versions of the NFSv4 protocol, there are two callback RPC
      procedures: CB_NULL and CB_COMPOUND.  The CB_COMPOUND procedure is
      defined in an analogous fashion to that of COMPOUND with its own set
      of callback operations.
    </t>

    <t>
      The addition of new server and callback operations within the
      COMPOUND and CB_COMPOUND request framework provides a means of
      extending the protocol in subsequent minor versions.
    </t>

    <t>
      Except for a small number of operations needed for session creation,
      server requests and callback requests are performed within the
      context of a session.  Sessions provide a client context for every
      request and support robust reply protection for non-idempotent
      requests.
    </t>
  </section>

  <section anchor="ss:core_infrastructure:CIaCO" title="Client Identifiers and Client Owners">
    <t>
      For each operation that obtains or depends on locking state, the
      specific client needs to be identifiable by the server.
    </t>

    <t>
      Each distinct client instance is represented by a client ID.  A
      client ID is a 64-bit identifier representing a specific client at a
      given time.  The client ID is changed whenever the client
      re-initializes, and may change when the server re-initializes.  Client
      IDs are used to support lock identification and crash recovery.
    </t>

    <t>
      During steady state operation, the client ID associated with each
      operation is derived from the session (see <xref target="ss:core_infrastructure:S" />) on which the
      operation is sent.  A session is associated with a client ID when the
      session is created.
    </t>

    <t>
      Unlike NFSv4.0, the only NFSv4.1 operations possible before a client
      ID is established are those needed to establish the client ID.
    </t>

    <t>
      A sequence of an EXCHANGE_ID operation followed by a CREATE_SESSION
      operation using that client ID (eir_clientid as returned from
      EXCHANGE_ID) is required to establish and confirm the client ID on
      the server.  Establishment of identification by a new incarnation of
      the client also has the effect of immediately releasing any locking
      state that a previous incarnation of that same client might have had
      on the server.  Such released state would include all byte-range
      lock, share reservation, layout state, and -- where the server
      supports neither the CLAIM_DELEGATE_PREV nor CLAIM_DELEG_CUR_FH claim
      types -- all delegation state associated with the same client with
      the same identity.  For discussion of delegation state recovery, see
      <xref target="ss:client-side_caching:DR" />.  For discussion of layout state recovery, see
      <xref target="ss:parallel_nfs:RfCR" />.
    </t>

    <t>
      Releasing such state requires that the server be able to determine
      that one client instance is the successor of another.  Where this
      cannot be done, for any of a number of reasons, the locking state
      will remain for a time subject to lease expiration (see <xref target="ss:state_management:LR" />)
      and the new client will need to wait for such state to be removed, if
      it makes conflicting lock requests.
    </t>

    <t>
      Client identification is encapsulated in the following client owner
      data type:
    </t>

    <?rfc include="autogen/type_client_owner4.xml"?>

    <t>
      The first field, co_verifier, is a client incarnation verifier.  The
      server will start the process of canceling the client's leased state
      if co_verifier is different than what the server has previously
      recorded for the identified client (as specified in the co_ownerid
      field).
    </t>

    <t>
      The second field, co_ownerid, is a variable length string that
      uniquely defines the client so that subsequent instances of the same
      client bear the same co_ownerid with a different verifier.
    </t>

    <t>
      There are several considerations for how the client generates the
      co_ownerid string:
    </t>

    <t>
      <list style='symbols'>
        <t>
          The string should be unique so that multiple clients do not
          present the same string.  The consequences of two clients
          presenting the same string range from one client getting an error
          to one client having its leased state abruptly and unexpectedly
          cancelled.
        </t>

        <t>
          The string should be selected so that subsequent incarnations
          (e.g., restarts) of the same client cause the client to present
          the same string.  The implementor is cautioned from an approach
          that requires the string to be recorded in a local file because
          this precludes the use of the implementation in an environment
          where there is no local disk and all file access is from an
          NFSv4.1 server.
        </t>

        <t>
          The string should be the same for each server network address that
          the client accesses.  This way, if a server has multiple
          interfaces, the client can trunk traffic over multiple network
          paths as described in <xref target="ss:core_infrastructure:T" />.  (Note: the precise opposite
          was advised in the NFSv4.0 specification <xref target="RFC3530" />.)
        </t>

        <t>
          The algorithm for generating the string should not assume that the
          client's network address will not change, unless the client
          implementation knows it is using statically assigned network
          addresses.  This includes changes between client incarnations and
          even changes while the client is still running in its current
          incarnation.  Thus, with dynamic address assignment, if the client
          includes just the client's network address in the co_ownerid
          string, there is a real risk that after the client gives up the
          network address, another client, using a similar algorithm for
          generating the co_ownerid string, would generate a conflicting
          co_ownerid string.
        </t>
      </list>
    </t>

    <t>
      Given the above considerations, an example of a well-generated
      co_ownerid string is one that includes:
    </t>

    <t>
      <list style='symbols'>
        <t>
          If applicable, the client's statically assigned network address.
        </t>

        <t>
          Additional information that tends to be unique, such as one or
          more of:

          <list style='symbols'>
            <t>
              The client machine's serial number (for privacy reasons, it is
              best to perform some one-way function on the serial number).
            </t>

            <t>
              A Media Access Control (MAC) address (again, a one-way function
              should be performed).
            </t>

            <t>
              The timestamp of when the NFSv4.1 software was first installed
              on the client (though this is subject to the previously
              mentioned caution about using information that is stored in a
              file, because the file might only be accessible over NFSv4.1).
            </t>

            <t>
              A true random number.  However, since this number ought to be
              the same between client incarnations, this shares the same
              problem as that of using the timestamp of the software
              installation.
            </t>
          </list>
        </t>

        <t>
          For a user-level NFSv4.1 client, it should contain additional
          information to distinguish the client from other user-level
          clients running on the same host, such as an universally unique
          identifier (UUID).
        </t>
      </list>
    </t>

    <t>
      The client ID is assigned by the server (the eir_clientid result from
      EXCHANGE_ID) and should be chosen so that it will not conflict with a
      client ID previously assigned by the server.  This applies across
      server restarts.
    </t>

    <t>
      In the event of a server restart, a client may find out that its
      current client ID is no longer valid when it receives an
      NFS4ERR_STALE_CLIENTID error.  The precise circumstances depend on
      the characteristics of the sessions involved, specifically whether
      the session is persistent (see <xref target="ss:core_infrastructure:P1" />), but in each case
      the client will receive this error when it attempts to establish a
      new session with the existing client ID and receives the error
      NFS4ERR_STALE_CLIENTID, indicating that a new client ID needs to be
      obtained via EXCHANGE_ID and the new session established with that
      client ID.
    </t>

    <t>
      When a session is not persistent, the client will find out that it
      needs to create a new session as a result of getting an
      NFS4ERR_BADSESSION, since the session in question was lost as part of
      a server restart.  When the existing client ID is presented to a
      server as part of creating a session and that client ID is not
      recognized, as would happen after a server restart, the server will
      reject the request with the error NFS4ERR_STALE_CLIENTID.
    </t>

    <t>
      In the case of the session being persistent, the client will
      re-establish communication using the existing session after the restart.
      This session will be associated with the existing client ID but may
      only be used to retransmit operations that the client previously
      transmitted and did not see replies to.  Replies to operations that
      the server previously performed will come from the reply cache;
      otherwise, NFS4ERR_DEADSESSION will be returned.  Hence, such a
      session is referred to as "dead".  In this situation, in order to
      perform new operations, the client needs to establish a new session.
      If an attempt is made to establish this new session with the existing
      client ID, the server will reject the request with
      NFS4ERR_STALE_CLIENTID.
    </t>

    <t>
      When NFS4ERR_STALE_CLIENTID is received in either of these
      situations, the client needs to obtain a new client ID by use of the
      EXCHANGE_ID operation, then use that client ID as the basis of a new
      session, and then proceed to any other necessary recovery for the
      server restart case (see <xref target="ss:state_management:SFaR" />).
    </t>

    <t>
      See the descriptions of EXCHANGE_ID (<xref target="op_EXCHANGE_ID" />) and
      CREATE_SESSION (<xref target="op_CREATE_SESSION" />) for a complete specification of these
      operations.
    </t>

    <section toc='exclude' anchor="ss:core_infrastructure:UfNtN" title="Upgrade from NFSv4.0 to NFSv4.1">
      <t>
        To facilitate upgrade from NFSv4.0 to NFSv4.1, a server may compare a
        value of data type client_owner4 in an EXCHANGE_ID with a value of
        data type nfs_client_id4 that was established using the SETCLIENTID
        operation of NFSv4.0.  A server that does so will allow an upgraded
        client to avoid waiting until the lease (i.e., the lease established
        by the NFSv4.0 instance client) expires.  This requires that the
        value of data type client_owner4 be constructed the same way as the
        value of data type nfs_client_id4.  If the latter's contents included
        the server's network address (per the recommendations of the NFSv4.0
        specification <xref target="RFC3530" />), and the NFSv4.1 client does not wish to use a
        client ID that prevents trunking, it should send two EXCHANGE_ID
        operations.  The first EXCHANGE_ID will have a client_owner4 equal to
        the nfs_client_id4.  This will clear the state created by the NFSv4.0
        client.  The second EXCHANGE_ID will not have the server's network
        address.  The state created for the second EXCHANGE_ID will not have
        to wait for lease expiration, because there will be no state to
        expire.
      </t>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:SRoCI" title="Server Release of Client ID">
      <t>
        NFSv4.1 introduces a new operation called DESTROY_CLIENTID
        (<xref target="op_DESTROY_CLIENTID" />), which the client SHOULD use to destroy a client ID
        it no longer needs.  This permits graceful, bilateral release of a
        client ID.  The operation cannot be used if there are sessions
        associated with the client ID, or state with an unexpired lease.
      </t>

      <t>
        If the server determines that the client holds no associated state
        for its client ID (associated state includes unrevoked sessions,
        opens, locks, delegations, layouts, and wants), the server MAY choose
        to unilaterally release the client ID in order to conserve resources.
        If the client contacts the server after this release, the server MUST
        ensure that the client receives the appropriate error so that it will
        use the EXCHANGE_ID/CREATE_SESSION sequence to establish a new client
        ID.  The server ought to be very hesitant to release a client ID
        since the resulting work on the client to recover from such an event
        will be the same burden as if the server had failed and restarted.
        Typically, a server would not release a client ID unless there had
        been no activity from that client for many minutes.  As long as there
        are sessions, opens, locks, delegations, layouts, or wants, the
        server MUST NOT release the client ID.  See <xref target="ss:core_infrastructure:LoS" /> for
        discussion on releasing inactive sessions.
      </t>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:RCOC" title="Resolving Client Owner Conflicts">
      <t>
        When the server gets an EXCHANGE_ID for a client owner that currently
        has no state, or that has state but the lease has expired, the server
        MUST allow the EXCHANGE_ID and confirm the new client ID if followed
        by the appropriate CREATE_SESSION.
      </t>

      <t>
        When the server gets an EXCHANGE_ID for a new incarnation of a client
        owner that currently has an old incarnation with state and an
        unexpired lease, the server is allowed to dispose of the state of the
        previous incarnation of the client owner if one of the following is
        true:
      </t>

      <t>
        <list style='symbols'>
          <t>
            The principal that created the client ID for the client owner is
            the same as the principal that is sending the EXCHANGE_ID
            operation.  Note that if the client ID was created with
            SP4_MACH_CRED state protection (<xref target="op_EXCHANGE_ID" />), the principal MUST
            be based on RPCSEC_GSS authentication, the RPCSEC_GSS service used
            MUST be integrity or privacy, and the same GSS mechanism and
            principal MUST be used as that used when the client ID was
            created.
          </t>

          <t>
            The client ID was established with SP4_SSV protection
            (<xref target="op_EXCHANGE_ID" />, <xref target="ss:core_infrastructure:PfUSC" />) and the client sends the
            EXCHANGE_ID with the security flavor set to RPCSEC_GSS using the
            GSS SSV mechanism (<xref target="ss:core_infrastructure:TSSVGM" />).
          </t>

          <t>
            The client ID was established with SP4_SSV protection, and under
            the conditions described herein, the EXCHANGE_ID was sent with
            SP4_MACH_CRED state protection.  Because the SSV might not persist
            across client and server restart, and because the first time a
            client sends EXCHANGE_ID to a server it does not have an SSV, the
            client MAY send the subsequent EXCHANGE_ID without an SSV
            RPCSEC_GSS handle.  Instead, as with SP4_MACH_CRED protection, the
            principal MUST be based on RPCSEC_GSS authentication, the
            RPCSEC_GSS service used MUST be integrity or privacy, and the same
            GSS mechanism and principal MUST be used as that used when the
            client ID was created.
          </t>
        </list>
      </t>

      <t>
        If none of the above situations apply, the server MUST return
        NFS4ERR_CLID_INUSE.
      </t>

      <t>
        If the server accepts the principal and co_ownerid as matching that
        which created the client ID, and the co_verifier in the EXCHANGE_ID
        differs from the co_verifier used when the client ID was created,
        then after the server receives a CREATE_SESSION that confirms the
        client ID, the server deletes state.  If the co_verifier values are
        the same (e.g., the client either is updating properties of the
        client ID (<xref target="op_EXCHANGE_ID" />) or is attempting trunking (<xref target="ss:core_infrastructure:T" />),
        the server MUST NOT delete state.
      </t>
    </section>
  </section>

  <section anchor="ss:core_infrastructure:SO" title="Server Owners">
    <t>
      The server owner is similar to a client owner (<xref target="ss:core_infrastructure:CIaCO" />), but
      unlike the client owner, there is no shorthand server ID.  The server
      owner is defined in the following data type:
    </t>

    <?rfc include="autogen/type_server_owner4.xml"?>

    <t>
      The server owner is returned from EXCHANGE_ID.  When the so_major_id
      fields are the same in two EXCHANGE_ID results, the connections that
      each EXCHANGE_ID were sent over can be assumed to address the same
      server (as defined in <xref target="ss:introduction:GD" />).  If the so_minor_id fields are
      also the same, then not only do both connections connect to the same
      server, but the session can be shared across both connections.  The
      reader is cautioned that multiple servers may deliberately or
      accidentally claim to have the same so_major_id or so_major_id/
      so_minor_id; the reader should examine
      Sections <xref target="ss:core_infrastructure:T" format='counter' /> and <xref target="op_EXCHANGE_ID" format='counter' /> in
      order to avoid acting on falsely matching server owner values.
    </t>

    <t>
      The considerations for generating a so_major_id are similar to that
      for generating a co_ownerid string (see <xref target="ss:core_infrastructure:CIaCO" />).  The
      consequences of two servers generating conflicting so_major_id values
      are less dire than they are for co_ownerid conflicts because the
      client can use RPCSEC_GSS to compare the authenticity of each server
      (see <xref target="ss:core_infrastructure:T" />).
    </t>
  </section>

  <section anchor="ss:core_infrastructure:SSN" title="Security Service Negotiation">
    <t>
      With the NFSv4.1 server potentially offering multiple security
      mechanisms, the client needs a method to determine or negotiate which
      mechanism is to be used for its communication with the server.  The
      NFS server may have multiple points within its file system namespace
      that are available for use by NFS clients.  These points can be
      considered security policy boundaries, and, in some NFS
      implementations, are tied to NFS export points.  In turn, the NFS
      server may be configured such that each of these security policy
      boundaries may have different or multiple security mechanisms in use.
    </t>

    <t>
      The security negotiation between client and server SHOULD be done
      with a secure channel to eliminate the possibility of a third party
      intercepting the negotiation sequence and forcing the client and
      server to choose a lower level of security than required or desired.
      See <xref target="sec:security_considerations" /> for further discussion.
    </t>

    <section toc='exclude' anchor="ss:core_infrastructure:NST" title="NFSv4.1 Security Tuples">
      <t>
        An NFS server can assign one or more "security tuples" to each
        security policy boundary in its namespace.  Each security tuple
        consists of a security flavor (see <xref target="ss:core_infrastructure:RSF" />) and, if the
        flavor is RPCSEC_GSS, a GSS-API mechanism Object Identifier (OID), a
        GSS-API quality of protection, and an RPCSEC_GSS service.
      </t>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:SaS" title="SECINFO and SECINFO_NO_NAME">
      <t>
        The SECINFO and SECINFO_NO_NAME operations allow the client to
        determine, on a per-filehandle basis, what security tuple is to be
        used for server access.  In general, the client will not have to use
        either operation except during initial communication with the server
        or when the client crosses security policy boundaries at the server.
        However, the server's policies may also change at any time and force
        the client to negotiate a new security tuple.
      </t>

      <t>
        Where the use of different security tuples would affect the type of
        access that would be allowed if a request was sent over the same
        connection used for the SECINFO or SECINFO_NO_NAME operation (e.g.,
        read-only vs. read-write) access, security tuples that allow greater
        access should be presented first.  Where the general level of access
        is the same and different security flavors limit the range of
        principals whose privileges are recognized (e.g., allowing or
        disallowing root access), flavors supporting the greatest range of
        principals should be listed first.
      </t>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:SE" title="Security Error">
      <t>
        Based on the assumption that each NFSv4.1 client and server MUST
        support a minimum set of security (i.e., Kerberos V5 under
        RPCSEC_GSS), the NFS client will initiate file access to the server
        with one of the minimal security tuples.  During communication with
        the server, the client may receive an NFS error of NFS4ERR_WRONGSEC.
        This error allows the server to notify the client that the security
        tuple currently being used contravenes the server's security policy.
        The client is then responsible for determining (see <xref target="ss:core_infrastructure:UNSaS" />)
        what security tuples are available at the server and choosing one
        that is appropriate for the client.
      </t>

      <section toc='exclude' anchor="ss:core_infrastructure:UNSaS" title="Using NFS4ERR_WRONGSEC, SECINFO, and SECINFO_NO_NAME">
        <t>
          This section explains the mechanics of NFSv4.1 security negotiation.
        </t>

        <section toc='exclude' anchor="ss:core_infrastructure:PFO" title="Put Filehandle Operations">
          <t>
            The term "put filehandle operation" refers to PUTROOTFH, PUTPUBFH,
            PUTFH, and RESTOREFH.  Each of the subsections herein describes how
            the server handles a subseries of operations that starts with a put
            filehandle operation.
          </t>

          <section toc='exclude' anchor="ss:core_infrastructure:PFOS" title="Put Filehandle Operation + SAVEFH">
            <t>
              The client is saving a filehandle for a future RESTOREFH, LINK, or
              RENAME.  SAVEFH MUST NOT return NFS4ERR_WRONGSEC.  To determine
              whether or not the put filehandle operation returns NFS4ERR_WRONGSEC,
              the server implementation pretends SAVEFH is not in the series of
              operations and examines which of the situations described in the
              other subsections of <xref target="ss:core_infrastructure:PFO" /> apply.
            </t>
          </section>

          <section toc='exclude' anchor="ss:core_infrastructure:ToMPFO" title="Two or More Put Filehandle Operations">
            <t>
              For a series of N put filehandle operations, the server MUST NOT
              return NFS4ERR_WRONGSEC to the first N-1 put filehandle operations.
              The Nth put filehandle operation is handled as if it is the first in
              a subseries of operations.  For example, if the server received a
              COMPOUND request with this series of operations -- PUTFH, PUTROOTFH,
              LOOKUP -- then the PUTFH operation is ignored for NFS4ERR_WRONGSEC
              purposes, and the PUTROOTFH, LOOKUP subseries is processed as
              according to <xref target="ss:core_infrastructure:PFOLOoaEN" />.
            </t>
          </section>

          <section toc='exclude' anchor="ss:core_infrastructure:PFOLOoaEN" title="Put Filehandle Operation + LOOKUP (or OPEN of an Existing Name)">
            <t>
              This situation also applies to a put filehandle operation followed by
              a LOOKUP or an OPEN operation that specifies an existing component
              name.
            </t>

            <t>
              In this situation, the client is potentially crossing a security
              policy boundary, and the set of security tuples the parent directory
              supports may differ from those of the child.  The server
              implementation may decide whether to impose any restrictions on
              security policy administration.  There are at least three approaches
              (sec_policy_child is the tuple set of the child export,
              sec_policy_parent is that of the parent).
            </t>

            <t>
              <list style='format (%c)'>
                <t>
                  sec_policy_child &lt;= sec_policy_parent (&lt;= for subset).  This
                  means that the set of security tuples specified on the security
                  policy of a child directory is always a subset of its parent
                  directory.
                </t>

                <t>
                  sec_policy_child ^ sec_policy_parent != {} (^ for intersection,
                  {} for the empty set).  This means that the set of security
                  tuples specified on the security policy of a child directory
                  always has a non-empty intersection with that of the parent.
                </t>

                <t>
                  sec_policy_child ^ sec_policy_parent == {}.  This means that the
                  set of security tuples specified on the security policy of a
                  child directory may not intersect with that of the parent.  In
                  other words, there are no restrictions on how the system
                  administrator may set up these tuples.
                </t>
              </list>
            </t>

            <t>
              In order for a server to support approaches (b) (for the case when a
              client chooses a flavor that is not a member of sec_policy_parent)
              and (c), the put filehandle operation cannot return NFS4ERR_WRONGSEC
              when there is a security tuple mismatch.  Instead, it should be
              returned from the LOOKUP (or OPEN by existing component name) that
              follows.
            </t>

            <t>
              Since the above guideline does not contradict approach (a), it should
              be followed in general.  Even if approach (a) is implemented, it is
              possible for the security tuple used to be acceptable for the target
              of LOOKUP but not for the filehandles used in the put filehandle
              operation.  The put filehandle operation could be a PUTROOTFH or
              PUTPUBFH, where the client cannot know the security tuples for the
              root or public filehandle.  Or the security policy for the filehandle
              used by the put filehandle operation could have changed since the
              time the filehandle was obtained.
            </t>

            <t>
              Therefore, an NFSv4.1 server MUST NOT return NFS4ERR_WRONGSEC in
              response to the put filehandle operation if the operation is
              immediately followed by a LOOKUP or an OPEN by component name.
            </t>
          </section>

          <section toc='exclude' anchor="ss:core_infrastructure:PFOL" title="Put Filehandle Operation + LOOKUPP">
            <t>
              Since SECINFO only works its way down, there is no way LOOKUPP can
              return NFS4ERR_WRONGSEC without SECINFO_NO_NAME.  SECINFO_NO_NAME
              solves this issue via style SECINFO_STYLE4_PARENT, which works in the
              opposite direction as SECINFO.  As with <xref target="ss:core_infrastructure:PFOLOoaEN" />, a put
              filehandle operation that is followed by a LOOKUPP MUST NOT return
              NFS4ERR_WRONGSEC.  If the server does not support SECINFO_NO_NAME,
              the client's only recourse is to send the put filehandle operation,
              LOOKUPP, GETFH sequence of operations with every security tuple it
              supports.
            </t>

            <t>
              Regardless of whether SECINFO_NO_NAME is supported, an NFSv4.1 server
              MUST NOT return NFS4ERR_WRONGSEC in response to a put filehandle
              operation if the operation is immediately followed by a LOOKUPP.
            </t>
          </section>

          <section toc='exclude' anchor="ss:core_infrastructure:PFOS1" title="Put Filehandle Operation + SECINFO/SECINFO_NO_NAME">
            <t>
              A security-sensitive client is allowed to choose a strong security
              tuple when querying a server to determine a file object's permitted
              security tuples.  The security tuple chosen by the client does not
              have to be included in the tuple list of the security policy of
              either the parent directory indicated in the put filehandle operation
              or the child file object indicated in SECINFO (or any parent
              directory indicated in SECINFO_NO_NAME).  Of course, the server has
              to be configured for whatever security tuple the client selects;
              otherwise, the request will fail at the RPC layer with an appropriate
              authentication error.
            </t>

            <t>
              In theory, there is no connection between the security flavor used by
              SECINFO or SECINFO_NO_NAME and those supported by the security
              policy.  But in practice, the client may start looking for strong
              flavors from those supported by the security policy, followed by
              those in the REQUIRED set.
            </t>

            <t>
              The NFSv4.1 server MUST NOT return NFS4ERR_WRONGSEC to a put
              filehandle operation that is immediately followed by SECINFO or
              SECINFO_NO_NAME.  The NFSv4.1 server MUST NOT return NFS4ERR_WRONGSEC
              from SECINFO or SECINFO_NO_NAME.
            </t>
          </section>

          <section toc='exclude' anchor="ss:core_infrastructure:PFON" title="Put Filehandle Operation + Nothing">
            <t>
              The NFSv4.1 server MUST NOT return NFS4ERR_WRONGSEC.
            </t>
          </section>

          <section toc='exclude' anchor="ss:core_infrastructure:PFOAE" title="Put Filehandle Operation + Anything Else">
            <t>
              "Anything Else" includes OPEN by filehandle.
            </t>

            <t>
              The security policy enforcement applies to the filehandle specified
              in the put filehandle operation.  Therefore, the put filehandle
              operation MUST return NFS4ERR_WRONGSEC when there is a security tuple
              mismatch.  This avoids the complexity of adding NFS4ERR_WRONGSEC as
              an allowable error to every other operation.
            </t>

            <t>
              A COMPOUND containing the series put filehandle operation +
              SECINFO_NO_NAME (style SECINFO_STYLE4_CURRENT_FH) is an efficient way
              for the client to recover from NFS4ERR_WRONGSEC.
            </t>

            <t>
              The NFSv4.1 server MUST NOT return NFS4ERR_WRONGSEC to any operation
              other than a put filehandle operation, LOOKUP, LOOKUPP, and OPEN (by
              component name).
            </t>
          </section>

          <section toc='exclude' anchor="ss:core_infrastructure:OaSaS" title="Operations after SECINFO and SECINFO_NO_NAME">
            <t>
              Suppose a client sends a COMPOUND procedure containing the series
              SEQUENCE, PUTFH, SECINFO_NONAME, READ, and suppose the security tuple
              used does not match that required for the target file.  By rule (see
              <xref target="ss:core_infrastructure:PFOS1" />), neither PUTFH nor SECINFO_NO_NAME can return
              NFS4ERR_WRONGSEC.  By rule (see <xref target="ss:core_infrastructure:PFOAE" />), READ cannot
              return NFS4ERR_WRONGSEC.  The issue is resolved by the fact that
              SECINFO and SECINFO_NO_NAME consume the current filehandle (note that
              this is a change from NFSv4.0).  This leaves no current filehandle
              for READ to use, and READ returns NFS4ERR_NOFILEHANDLE.
            </t>
          </section>
        </section>

        <section toc='exclude' anchor="ss:core_infrastructure:LaR" title="LINK and RENAME">
          <t>
            The LINK and RENAME operations use both the current and saved
            filehandles.  Technically, the server MAY return NFS4ERR_WRONGSEC
            from LINK or RENAME if the security policy of the saved filehandle
            rejects the security flavor used in the COMPOUND request's
            credentials.  If the server does so, then if there is no intersection
            between the security policies of saved and current filehandles, this
            means that it will be impossible for the client to perform the
            intended LINK or RENAME operation.
          </t>

          <t>
            For example, suppose the client sends this COMPOUND request:
            SEQUENCE, PUTFH bFH, SAVEFH, PUTFH aFH, RENAME "c" "d", where
            filehandles bFH and aFH refer to different directories.  Suppose no
            common security tuple exists between the security policies of aFH and
            bFH.  If the client sends the request using credentials acceptable to
            bFH's security policy but not aFH's policy, then the PUTFH aFH
            operation will fail with NFS4ERR_WRONGSEC.  After a SECINFO_NO_NAME
            request, the client sends SEQUENCE, PUTFH bFH, SAVEFH, PUTFH aFH,
            RENAME "c" "d", using credentials acceptable to aFH's security policy
            but not bFH's policy.  The server returns NFS4ERR_WRONGSEC on the
            RENAME operation.
          </t>

          <t>
            To prevent a client from an endless sequence of a request containing
            LINK or RENAME, followed by a request containing SECINFO_NO_NAME or
            SECINFO, the server MUST detect when the security policies of the
            current and saved filehandles have no mutually acceptable security
            tuple, and MUST NOT return NFS4ERR_WRONGSEC from LINK or RENAME in
            that situation.  Instead the server MUST do one of two things:
          </t>

          <t>
            <list style='symbols'>
              <t>
                The server can return NFS4ERR_XDEV.
              </t>

              <t>
                The server can allow the security policy of the current filehandle
                to override that of the saved filehandle, and so return NFS4_OK.
              </t>
            </list>
          </t>
        </section>
      </section>
    </section>
  </section>

  <section anchor="ss:core_infrastructure:MV" title="Minor Versioning">
    <t>
      To address the requirement of an NFS protocol that can evolve as the
      need arises, the NFSv4.1 protocol contains the rules and framework to
      allow for future minor changes or versioning.
    </t>

    <t>
      The base assumption with respect to minor versioning is that any
      future accepted minor version will be documented in one or more
      Standards Track RFCs.  Minor version 0 of the NFSv4 protocol is
      represented by <xref target="RFC3530" />, and minor version 1 is represented by this RFC.
      The COMPOUND and CB_COMPOUND procedures support the encoding of the
      minor version being requested by the client.
    </t>

    <t>
      The following items represent the basic rules for the development of
      minor versions.  Note that a future minor version may modify or add
      to the following rules as part of the minor version definition.
    </t>

    <t>
      <list style='numbers'>
        <t>
          Procedures are not added or deleted.
          <vspace blankLines="1"/>
          To maintain the general RPC model, NFSv4 minor versions will not
          add to or delete procedures from the NFS program.
        </t>

        <t>
          Minor versions may add operations to the COMPOUND and
          CB_COMPOUND procedures.
          <vspace blankLines="1"/>
          The addition of operations to the COMPOUND and CB_COMPOUND
          procedures does not affect the RPC model.
          <vspace blankLines="1"/>
          <list style='symbols'>
            <t>
              Minor versions may append attributes to the bitmap4 that
              represents sets of attributes and to the fattr4 that
              represents sets of attribute values.
              <vspace blankLines="1"/>
              This allows for the expansion of the attribute model to allow
              for future growth or adaptation.
            </t>

            <t>
              Minor version X must append any new attributes after the last
              documented attribute.
              <vspace blankLines="1"/>
              Since attribute results are specified as an opaque array of
              per-attribute, XDR-encoded results, the complexity of adding
              new attributes in the midst of the current definitions would
              be too burdensome.
            </t>
          </list>
        </t>

        <t>
          Minor versions must not modify the structure of an existing
          operation's arguments or results.
          <vspace blankLines="1"/>
          Again, the complexity of handling multiple structure definitions
          for a single operation is too burdensome.  New operations should
          be added instead of modifying existing structures for a minor
          version.
          <vspace blankLines="1"/>
          This rule does not preclude the following adaptations in a minor
          version:
          <vspace blankLines="1"/>
          <list style='symbols'>
            <t>
              adding bits to flag fields, such as new attributes to
              GETATTR's bitmap4 data type, and providing corresponding
              variants of opaque arrays, such as a notify4 used together
              with such bitmaps
            </t>

            <t>
              adding bits to existing attributes like ACLs that have flag
              words
            </t>

            <t>
              extending enumerated types (including NFS4ERR_*) with new
              values
            </t>

            <t>
              adding cases to a switched union
            </t>
          </list>
        </t>

        <t>
          Minor versions must not modify the structure of existing
          attributes.
        </t>

        <t>
          Minor versions must not delete operations.
            <vspace blankLines="1"/>
          This prevents the potential reuse of a particular operation
          "slot" in a future minor version.
        </t>

        <t>
          Minor versions must not delete attributes.
        </t>

        <t>
          Minor versions must not delete flag bits or enumeration values.
        </t>

        <t>
          Minor versions may declare an operation MUST NOT be implemented.
          <vspace blankLines="1"/>
          Specifying that an operation MUST NOT be implemented is
          equivalent to obsoleting an operation.  For the client, it means
          that the operation MUST NOT be sent to the server.  For the
          server, an NFS error can be returned as opposed to "dropping"
          the request as an XDR decode error.  This approach allows for
          the obsolescence of an operation while maintaining its structure
          so that a future minor version can reintroduce the operation.
          <vspace blankLines="1"/>
          <list style='numbers'>
            <t>
              Minor versions may declare that an attribute MUST NOT be
              implemented.
            </t>

            <t>
              Minor versions may declare that a flag bit or enumeration
              value MUST NOT be implemented.
            </t>
          </list>
        </t>

        <t>
          Minor versions may downgrade features from REQUIRED to
          RECOMMENDED, or RECOMMENDED to OPTIONAL.
        </t>

        <t>
          Minor versions may upgrade features from OPTIONAL to
          RECOMMENDED, or RECOMMENDED to REQUIRED.
        </t>

        <t>
          A client and server that support minor version X SHOULD support
          minor versions 0 through X-1 as well.
        </t>

        <t>
          Except for infrastructural changes, a minor version must not
          introduce REQUIRED new features.
            <vspace blankLines="1"/>
          This rule allows for the introduction of new functionality and
          forces the use of implementation experience before designating a
          feature as REQUIRED.  On the other hand, some classes of
          features are infrastructural and have broad effects.  Allowing
          infrastructural features to be RECOMMENDED or OPTIONAL
          complicates implementation of the minor version.
        </t>

        <t>
          A client MUST NOT attempt to use a stateid, filehandle, or
          similar returned object from the COMPOUND procedure with minor
          version X for another COMPOUND procedure with minor version Y,
          where X != Y.
        </t>
      </list>
    </t>
  </section>

  <section anchor="ss:core_infrastructure:NSS" title="Non-RPC-Based Security Services">
    <t>
      As described in <xref target="ss:core_infrastructure:IAIP" />, NFSv4.1 relies on RPC for
      identification, authentication, integrity, and privacy.  NFSv4.1
      itself provides or enables additional security services as described
      in the next several subsections.
    </t>

    <section toc='exclude' anchor="ss:core_infrastructure:A" title="Authorization">
      <t>
        Authorization to access a file object via an NFSv4.1 operation is
        ultimately determined by the NFSv4.1 server.  A client can
        predetermine its access to a file object via the OPEN (<xref target="op_OPEN" />)
        and the ACCESS (<xref target="op_ACCESS" />) operations.
      </t>

      <t>
        Principals with appropriate access rights can modify the
        authorization on a file object via the SETATTR (<xref target="op_SETATTR" />)
        operation.  Attributes that affect access rights include mode, owner,
        owner_group, acl, dacl, and sacl.  See <xref target="sec:file_attributes" />.
      </t>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:A1" title="Auditing">
      <t>
        NFSv4.1 provides auditing on a per-file object basis, via the acl and
        sacl attributes as described in <xref target="sec:access_control" />.  It is outside the scope
        of this specification to specify audit log formats or management
        policies.
      </t>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:ID" title="Intrusion Detection">
      <t>
        NFSv4.1 provides alarm control on a per-file object basis, via the
        acl and sacl attributes as described in <xref target="sec:access_control" />.  Alarms may serve
        as the basis for intrusion detection.  It is outside the scope of
        this specification to specify heuristics for detecting intrusion via
        alarms.
      </t>
    </section>
  </section>

  <section anchor="ss:core_infrastructure:TL" title="Transport Layers">
    <section toc='exclude' anchor="ss:core_infrastructure:RaRPoT" title="REQUIRED and RECOMMENDED Properties of Transports">
      <t>
        NFSv4.1 works over Remote Direct Memory Access (RDMA) and
        non-RDMA-based transports with the following attributes:
      </t>

      <t>
        <list style='symbols'>
          <t>
            The transport supports reliable delivery of data, which NFSv4.1
            requires but neither NFSv4.1 nor RPC has facilities for ensuring
            <xref target="Chet" />.
          </t>

          <t>
            The transport delivers data in the order it was sent.  Ordered
            delivery simplifies detection of transmit errors, and simplifies
            the sending of arbitrary sized requests and responses via the
            record marking protocol <xref target="RFC5531" />.
          </t>
        </list>
      </t>

      <t>
        Where an NFSv4.1 implementation supports operation over the IP
        network protocol, any transport used between NFS and IP MUST be among
        the IETF-approved congestion control transport protocols.  At the
        time this document was written, the only two transports that had the
        above attributes were TCP and the Stream Control Transmission
        Protocol (SCTP).  To enhance the possibilities for interoperability,
        an NFSv4.1 implementation MUST support operation over the TCP
        transport protocol.
      </t>

      <t>
        Even if NFSv4.1 is used over a non-IP network protocol, it is
        RECOMMENDED that the transport support congestion control.
      </t>

      <t>
        It is permissible for a connectionless transport to be used under
        NFSv4.1; however, reliable and in-order delivery of data combined
        with congestion control by the connectionless transport is REQUIRED.
        As a consequence, UDP by itself MUST NOT be used as an NFSv4.1
        transport.  NFSv4.1 assumes that a client transport address and
        server transport address used to send data over a transport together
        constitute a connection, even if the underlying transport eschews the
        concept of a connection.
      </t>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:CaSTB" title="Client and Server Transport Behavior">
      <t>
        If a connection-oriented transport (e.g., TCP) is used, the client
        and server SHOULD use long-lived connections for at least three
        reasons:
      </t>

      <t>
        <list style='numbers'>
          <t>
            This will prevent the weakening of the transport's congestion
            control mechanisms via short-lived connections.
          </t>

          <t>
            This will improve performance for the WAN environment by
            eliminating the need for connection setup handshakes.
          </t>

          <t>
            The NFSv4.1 callback model differs from NFSv4.0, and requires the
            client and server to maintain a client-created backchannel (see
            <xref target="ss:core_infrastructure:AoCCaS" />) for the server to use.
          </t>
        </list>
      </t>

      <t>
        In order to reduce congestion, if a connection-oriented transport is
        used, and the request is not the NULL procedure:
      </t>

      <t>
        <list style='symbols'>
          <t>
            A requester MUST NOT retry a request unless the connection the
            request was sent over was lost before the reply was received.
          </t>

          <t>
            A replier MUST NOT silently drop a request, even if the request is
            a retry.  (The silent drop behavior of RPCSEC_GSS <xref target="RFC2203" /> does not
            apply because this behavior happens at the RPCSEC_GSS layer, a
            lower layer in the request processing.)  Instead, the replier
            SHOULD return an appropriate error (see <xref target="ss:core_infrastructure:SIaRC" />), or it
            MAY disconnect the connection.
          </t>
        </list>
      </t>

      <t>
        When sending a reply, the replier MUST send the reply to the same
        full network address (e.g., if using an IP-based transport, the
        source port of the requester is part of the full network address)
        from which the requester sent the request.  If using a
        connection-oriented transport, replies MUST be sent on the same connection from
        which the request was received.
      </t>

      <t>
        If a connection is dropped after the replier receives the request but
        before the replier sends the reply, the replier might have a pending
        reply.  If a connection is established with the same source and
        destination full network address as the dropped connection, then the
        replier MUST NOT send the reply until the requester retries the
        request.  The reason for this prohibition is that the requester MAY
        retry a request over a different connection (provided that connection
        is associated with the original request's session).
      </t>

      <t>
        When using RDMA transports, there are other reasons for not
        tolerating retries over the same connection:
      </t>

      <t>
        <list style='symbols'>
          <t>
            RDMA transports use "credits" to enforce flow control, where a
            credit is a right to a peer to transmit a message.  If one peer
            were to retransmit a request (or reply), it would consume an
            additional credit.  If the replier retransmitted a reply, it would
            certainly result in an RDMA connection loss, since the requester
            would typically only post a single receive buffer for each
            request.  If the requester retransmitted a request, the additional
            credit consumed on the server might lead to RDMA connection
            failure unless the client accounted for it and decreased its
            available credit, leading to wasted resources.
          </t>

          <t>
            RDMA credits present a new issue to the reply cache in NFSv4.1.
            The reply cache may be used when a connection within a session is
            lost, such as after the client reconnects.  Credit information is
            a dynamic property of the RDMA connection, and stale values must
            not be replayed from the cache.  This implies that the reply cache
            contents must not be blindly used when replies are sent from it,
            and credit information appropriate to the channel must be
            refreshed by the RPC layer.
          </t>
        </list>
      </t>

      <t>
        In addition, as described in <xref target="ss:core_infrastructure:RaRoR" />, while a session is
        active, the NFSv4.1 requester MUST NOT stop waiting for a reply.
      </t>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:P" title="Ports">
      <t>
        Historically, NFSv3 servers have listened over TCP port 2049.  The
        registered port 2049 <xref target="RFC3232" /> for the NFS protocol should be the default
        configuration.  NFSv4.1 clients SHOULD NOT use the RPC binding
        protocols as described in <xref target="RFC1833" />.
      </t>
    </section>
  </section>

  <section anchor="ss:core_infrastructure:S" title="Session">
    <t>
      NFSv4.1 clients and servers MUST support and MUST use the session
      feature as described in this section.
    </t>

    <section toc='exclude' anchor="ss:core_infrastructure:MaO" title="Motivation and Overview">
      <t>
        Previous versions and minor versions of NFS have suffered from the
        following:
      </t>

      <t>
        <list style='symbols'>
          <t>
            Lack of support for Exactly Once Semantics (EOS).  This includes
            lack of support for EOS through server failure and recovery.
          </t>

          <t>
            Limited callback support, including no support for sending
            callbacks through firewalls, and races between replies to normal
            requests and callbacks.
          </t>

          <t>
            Limited trunking over multiple network paths.
          </t>

          <t>
            Requiring machine credentials for fully secure operation.
          </t>
        </list>
      </t>

      <t>
        Through the introduction of a session, NFSv4.1 addresses the above
        shortfalls with practical solutions:
      </t>

      <t>
        <list style='symbols'>
          <t>
            EOS is enabled by a reply cache with a bounded size, making it
            feasible to keep the cache in persistent storage and enable EOS
            through server failure and recovery.  One reason that previous
            revisions of NFS did not support EOS was because some EOS
            approaches often limited parallelism.  As will be explained in
            <xref target="ss:core_infrastructure:EOS" />, NFSv4.1 supports both EOS and unlimited
            parallelism.
          </t>

          <t>
            The NFSv4.1 client (defined in <xref target="ss:introduction:GD" />, <xref target='para:client_def' />) creates
            transport connections and provides them to the server to use for
            sending callback requests, thus solving the firewall issue
            (<xref target="op_BIND_CONN_TO_SESSION" />).  Races between responses from client requests and
            callbacks caused by the requests are detected via the session's
            sequencing properties that are a consequence of EOS
            (<xref target="ss:core_infrastructure:RSCR" />).
          </t>

          <t>
            The NFSv4.1 client can associate an arbitrary number of
            connections with the session, and thus provide trunking
            (<xref target="ss:core_infrastructure:T" />).
          </t>

          <t>
            The NFSv4.1 client and server produces a session key independent
            of client and server machine credentials which can be used to
            compute a digest for protecting critical session management
            operations (<xref target="ss:core_infrastructure:PfUSC" />).
          </t>

          <t>
            The NFSv4.1 client can also create secure RPCSEC_GSS contexts for
            use by the session's backchannel that do not require the server to
            authenticate to a client machine principal (<xref target="ss:core_infrastructure:BRS" />).
          </t>
        </list>
      </t>

      <t>
        A session is a dynamically created, long-lived server object created
        by a client and used over time from one or more transport
        connections.  Its function is to maintain the server's state relative
        to the connection(s) belonging to a client instance.  This state is
        entirely independent of the connection itself, and indeed the state
        exists whether or not the connection exists.  A client may have one
        or more sessions associated with it so that client-associated state
        may be accessed using any of the sessions associated with that
        client's client ID, when connections are associated with those
        sessions.  When no connections are associated with any of a client
        ID's sessions for an extended time, such objects as locks, opens,
        delegations, layouts, etc. are subject to expiration.  The session
        serves as an object representing a means of access by a client to the
        associated client state on the server, independent of the physical
        means of access to that state.
      </t>

      <t>
        A single client may create multiple sessions.  A single session MUST
        NOT serve multiple clients.
      </t>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:NI" title="NFSv4 Integration">
      <t>
        Sessions are part of NFSv4.1 and not NFSv4.0.  Normally, a major
        infrastructure change such as sessions would require a new major
        version number to an Open Network Computing (ONC) RPC program like
        NFS.  However, because NFSv4 encapsulates its functionality in a
        single procedure, COMPOUND, and because COMPOUND can support an
        arbitrary number of operations, sessions have been added to NFSv4.1
        with little difficulty.  COMPOUND includes a minor version number
        field, and for NFSv4.1 this minor version is set to 1.  When the
        NFSv4 server processes a COMPOUND with the minor version set to 1, it
        expects a different set of operations than it does for NFSv4.0.
      </t>

      <t>
        NFSv4.1 defines the SEQUENCE operation, which is required for every
        COMPOUND that operates over an established session, with the
        exception of some session administration operations, such as
        DESTROY_SESSION (<xref target="op_DESTROY_SESSION" />).
      </t>

      <section toc='exclude' anchor="ss:core_infrastructure:SaC" title="SEQUENCE and CB_SEQUENCE">
        <t>
          In NFSv4.1, when the SEQUENCE operation is present, it MUST be the
          first operation in the COMPOUND procedure.  The primary purpose of
          SEQUENCE is to carry the session identifier.  The session identifier
          associates all other operations in the COMPOUND procedure with a
          particular session.  SEQUENCE also contains required information for
          maintaining EOS (see <xref target="ss:core_infrastructure:EOS" />).  Session-enabled NFSv4.1
          COMPOUND requests thus have the form:
        </t>

        <figure>
          <artwork>
       +-----+--------------+-----------+------------+-----------+----
       | tag | minorversion | numops    |SEQUENCE op | op + args | ...
       |     |   (== 1)     | (limited) |  + args    |           |
       +-----+--------------+-----------+------------+-----------+----
          </artwork>
        </figure>

        <t>
          and the replies have the form:
        </t>

        <figure>
          <artwork>
       +------------+-----+--------+-------------------------------+--//
       |last status | tag | numres |status + SEQUENCE op + results |  //
       +------------+-----+--------+-------------------------------+--//
               //-----------------------+----
               // status + op + results | ...
               //-----------------------+----
          </artwork>
        </figure>

        <t>
          A CB_COMPOUND procedure request and reply has a similar form to
          COMPOUND, but instead of a SEQUENCE operation, there is a CB_SEQUENCE
          operation.  CB_COMPOUND also has an additional field called
          "callback_ident", which is superfluous in NFSv4.1 and MUST be ignored
          by the client.  CB_SEQUENCE has the same information as SEQUENCE, and
          also includes other information needed to resolve callback races
          (<xref target="ss:core_infrastructure:RSCR" />).
        </t>
      </section>

      <section toc='exclude' anchor="ss:core_infrastructure:CIaSA" title="Client ID and Session Association">
        <t>
          Each client ID (<xref target="ss:core_infrastructure:CIaCO" />) can have zero or more active sessions.
          A client ID and associated session are required to perform file
          access in NFSv4.1.  Each time a session is used (whether by a client
          sending a request to the server or the client replying to a callback
          request from the server), the state leased to its associated client
          ID is automatically renewed.
        </t>

        <t>
          State (which can consist of share reservations, locks, delegations,
          and layouts (<xref target="ss:introduction:LF" />)) is tied to the client ID.  Client state
          is not tied to any individual session.  Successive state changing
          operations from a given state owner MAY go over different sessions,
          provided the session is associated with the same client ID.  A
          callback MAY arrive over a different session than that of the request
          that originally acquired the state pertaining to the callback.  For
          example, if session A is used to acquire a delegation, a request to
          recall the delegation MAY arrive over session B if both sessions are
          associated with the same client ID.
          Sections <xref target="ss:core_infrastructure:SCS" format='counter' /> and <xref target="ss:core_infrastructure:BRS" format='counter' />
          discuss the security considerations around callbacks.
        </t>
      </section>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:C" title="Channels">
      <t>
        A channel is not a connection.  A channel represents the direction
        ONC RPC requests are sent.
      </t>

      <t>
        Each session has one or two channels: the fore channel and the
        backchannel.  Because there are at most two channels per session, and
        because each channel has a distinct purpose, channels are not
        assigned identifiers.
      </t>

      <t>
        The fore channel is used for ordinary requests from the client to the
        server, and carries COMPOUND requests and responses.  A session
        always has a fore channel.
      </t>

      <t>
        The backchannel is used for callback requests from server to client,
        and carries CB_COMPOUND requests and responses.  Whether or not there
        is a backchannel is a decision made by the client; however, many
        features of NFSv4.1 require a backchannel.  NFSv4.1 servers MUST
        support backchannels.
      </t>

      <t>
        Each session has resources for each channel, including separate reply
        caches (see <xref target="ss:core_infrastructure:SIaRC" />).  Note that even the backchannel
        requires a reply cache (or, at least, a slot table in order to detect
        retries) because some callback operations are nonidempotent.
      </t>

      <section toc='exclude' anchor="ss:core_infrastructure:AoCCaS" title="Association of Connections, Channels, and Sessions">
        <t>
          Each channel is associated with zero or more transport connections
          (whether of the same transport protocol or different transport
          protocols).  A connection can be associated with one channel or both
          channels of a session; the client and server negotiate whether a
          connection will carry traffic for one channel or both channels via
          the CREATE_SESSION (<xref target="op_CREATE_SESSION" />) and the BIND_CONN_TO_SESSION
          (<xref target="op_BIND_CONN_TO_SESSION" />) operations.  When a session is created via
          CREATE_SESSION, the connection that transported the CREATE_SESSION
          request is automatically associated with the fore channel, and
          optionally the backchannel.  If the client specifies no state
          protection (<xref target="op_EXCHANGE_ID" />) when the session is created, then when
          SEQUENCE is transmitted on a different connection, the connection is
          automatically associated with the fore channel of the session
          specified in the SEQUENCE operation.
        </t>

        <t>
          A connection's association with a session is not exclusive.  A
          connection associated with the channel(s) of one session may be
          simultaneously associated with the channel(s) of other sessions
          including sessions associated with other client IDs.
        </t>

        <t>
          It is permissible for connections of multiple transport types to be
          associated with the same channel.  For example, both TCP and RDMA
          connections can be associated with the fore channel.  In the event an
          RDMA and non-RDMA connection are associated with the same channel,
          the maximum number of slots SHOULD be at least one more than the
          total number of RDMA credits (<xref target="ss:core_infrastructure:SIaRC" />).  This way, if all
          RDMA credits are used, the non-RDMA connection can have at least one
          outstanding request.  If a server supports multiple transport types,
          it MUST allow a client to associate connections from each transport
          to a channel.
        </t>

        <t>
          It is permissible for a connection of one type of transport to be
          associated with the fore channel, and a connection of a different
          type to be associated with the backchannel.
        </t>
      </section>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:SS" title="Server Scope">
      <t>
        Servers each specify a server scope value in the form of an opaque
        string eir_server_scope returned as part of the results of an
        EXCHANGE_ID operation.  The purpose of the server scope is to allow a
        group of servers to indicate to clients that a set of servers sharing
        the same server scope value has arranged to use compatible values of
        otherwise opaque identifiers.  Thus, the identifiers generated by one
        server of that set may be presented to another of that same scope.
      </t>

      <t>
        The use of such compatible values does not imply that a value
        generated by one server will always be accepted by another.  In most
        cases, it will not.  However, a server will not accept a value
        generated by another inadvertently.  When it does accept it, it will
        be because it is recognized as valid and carrying the same meaning as
        on another server of the same scope.
      </t>

      <t>
        When servers are of the same server scope, this compatibility of
        values applies to the follow identifiers:
      </t>

      <t>
        <list style='symbols'>
          <t>
            Filehandle values.  A filehandle value accepted by two servers of
            the same server scope denotes the same object.  A WRITE operation
            sent to one server is reflected immediately in a READ sent to the
            other, and locks obtained on one server conflict with those
            requested on the other.
          </t>

          <t>
            Session ID values.  A session ID value accepted by two servers of
            the same server scope denotes the same session.
          </t>

          <t>
            Client ID values.  A client ID value accepted as valid by two
            servers of the same server scope is associated with two clients
            with the same client owner and verifier.
          </t>

          <t>
            State ID values.  A state ID value is recognized as valid when the
            corresponding client ID is recognized as valid.  If the same
            stateid value is accepted as valid on two servers of the same
            scope and the client IDs on the two servers represent the same
            client owner and verifier, then the two stateid values designate
            the same set of locks and are for the same file.
          </t>

          <t>
            Server owner values.  When the server scope values are the same,
            server owner value may be validly compared.  In cases where the
            server scope values are different, server owner values are treated
            as different even if they contain all identical bytes.
          </t>
        </list>
      </t>

      <t>
        The coordination among servers required to provide such compatibility
        can be quite minimal, and limited to a simple partition of the ID
        space.  The recognition of common values requires additional
        implementation, but this can be tailored to the specific situations
        in which that recognition is desired.
      </t>

      <t>
        Clients will have occasion to compare the server scope values of
        multiple servers under a number of circumstances, each of which will
        be discussed under the appropriate functional section:
      </t>

      <t>
        <list style='symbols'>
          <t>
            When server owner values received in response to EXCHANGE_ID
            operations sent to multiple network addresses are compared for the
            purpose of determining the validity of various forms of trunking,
            as described in <xref target="ss:core_infrastructure:T" />.
          </t>

          <t>
            When network or server reconfiguration causes the same network
            address to possibly be directed to different servers, with the
            necessity for the client to determine when lock reclaim should be
            attempted, as described in <xref target="ss:state_management:SR" />.
          </t>

          <t>
            When file system migration causes the transfer of responsibility
            for a file system between servers and the client needs to
            determine whether state has been transferred with the file system
            (as described in <xref target="ss:multi-server_namespace:LSaFST" />) or whether the client needs to
            reclaim state on a similar basis as in the case of server restart,
            as described in <xref target="ss:state_management:SFaR" />.
          </t>
        </list>
      </t>

      <t>
        When two replies from EXCHANGE_ID, each from two different server
        network addresses, have the same server scope, there are a number of
        ways a client can validate that the common server scope is due to two
        servers cooperating in a group.
      </t>

      <t>
        <list style='symbols'>
          <t>
            If both EXCHANGE_ID requests were sent with RPCSEC_GSS
            authentication and the server principal is the same for both
            targets, the equality of server scope is validated.  It is
            RECOMMENDED that two servers intending to share the same server
            scope also share the same principal name.
          </t>

          <t>
            The client may accept the appearance of the second server in the
            fs_locations or fs_locations_info attribute for a relevant file
            system.  For example, if there is a migration event for a
            particular file system or there are locks to be reclaimed on a
            particular file system, the attributes for that particular file
            system may be used.  The client sends the GETATTR request to the
            first server for the fs_locations or fs_locations_info attribute
            with RPCSEC_GSS authentication.  It may need to do this in advance
            of the need to verify the common server scope.  If the client
            successfully authenticates the reply to GETATTR, and the GETATTR
            request and reply containing the fs_locations or fs_locations_info
            attribute refers to the second server, then the equality of server
            scope is supported.  A client may choose to limit the use of this
            form of support to information relevant to the specific file
            system involved (e.g., a file system being migrated).
          </t>
        </list>
      </t>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:T" title="Trunking">
      <t>
        Trunking is the use of multiple connections between a client and
        server in order to increase the speed of data transfer.  NFSv4.1
        supports two types of trunking: session trunking and client ID
        trunking.
      </t>

      <t>
        NFSv4.1 servers MUST support both forms of trunking within the
        context of a single server network address and MUST support both
        forms within the context of the set of network addresses used to
        access a single server.  NFSv4.1 servers in a clustered configuration
        MAY allow network addresses for different servers to use client ID
        trunking.
      </t>

      <t>
        Clients may use either form of trunking as long as they do not, when
        trunking between different server network addresses, violate the
        servers' mandates as to the kinds of trunking to be allowed (see
        below).  With regard to callback channels, the client MUST allow the
        server to choose among all callback channels valid for a given client
        ID and MUST support trunking when the connections supporting the
        backchannel allow session or client ID trunking to be used for
        callbacks.
      </t>

      <t>
        Session trunking is essentially the association of multiple
        connections, each with potentially different target and/or source
        network addresses, to the same session.  When the target network
        addresses (server addresses) of the two connections are the same, the
        server MUST support such session trunking.  When the target network
        addresses are different, the server MAY indicate such support using
        the data returned by the EXCHANGE_ID operation (see below).
      </t>

      <t>
        Client ID trunking is the association of multiple sessions to the
        same client ID.  Servers MUST support client ID trunking for two
        target network addresses whenever they allow session trunking for
        those same two network addresses.  In addition, a server MAY, by
        presenting the same major server owner ID (<xref target="ss:core_infrastructure:SO" />) and server
        scope (<xref target="ss:core_infrastructure:SS" />), allow an additional case of client ID
        trunking.  When two servers return the same major server owner and
        server scope, it means that the two servers are cooperating on
        locking state management, which is a prerequisite for client ID
        trunking.
      </t>

      <t>
        Distinguishing when the client is allowed to use session and client
        ID trunking requires understanding how the results of the EXCHANGE_ID
        (<xref target="op_EXCHANGE_ID" />) operation identify a server.  Suppose a client sends
        EXCHANGE_IDs over two different connections, each with a possibly
        different target network address, but each EXCHANGE_ID operation has
        the same value in the eia_clientowner field.  If the same NFSv4.1
        server is listening over each connection, then each EXCHANGE_ID
        result MUST return the same values of eir_clientid,
        eir_server_owner.so_major_id, and eir_server_scope.  The client can
        then treat each connection as referring to the same server (subject
        to verification; see <xref target="ss:core_infrastructure:VCoMSI" /> later in this section), and it
        can use each connection to trunk requests and replies.  The client's
        choice is whether session trunking or client ID trunking applies.
      </t>

      <t>
        <list style='hanging'>
          <t hangText='Session Trunking.'>
            If the eia_clientowner argument is the same in two
            different EXCHANGE_ID requests, and the eir_clientid,
            eir_server_owner.so_major_id, eir_server_owner.so_minor_id, and
            eir_server_scope results match in both EXCHANGE_ID results, then
            the client is permitted to perform session trunking.  If the
            client has no session mapping to the tuple of eir_clientid,
            eir_server_owner.so_major_id, eir_server_scope, and
            eir_server_owner.so_minor_id, then it creates the session via a
            CREATE_SESSION operation over one of the connections, which
            associates the connection to the session.  If there is a session
            for the tuple, the client can send BIND_CONN_TO_SESSION to
            associate the connection to the session.
            <vspace blankLines="1"/>
            Of course, if the client does not desire to use session trunking,
            it is not required to do so.  It can invoke CREATE_SESSION on the
            connection.  This will result in client ID trunking as described
            below.  It can also decide to drop the connection if it does not
            choose to use trunking.
          </t>

          <t hangText='Client ID Trunking.'>
            If the eia_clientowner argument is the same in
            two different EXCHANGE_ID requests, and the eir_clientid,
            eir_server_owner.so_major_id, and eir_server_scope results match
            in both EXCHANGE_ID results, then the client is permitted to
            perform client ID trunking (regardless of whether the
            eir_server_owner.so_minor_id results match).  The client can
            associate each connection with different sessions, where each
            session is associated with the same server.
            <vspace blankLines="1"/>
            The client completes the act of client ID trunking by invoking
            CREATE_SESSION on each connection, using the same client ID that
            was returned in eir_clientid.  These invocations create two
            sessions and also associate each connection with its respective
            session.  The client is free to decline to use client ID trunking
            by simply dropping the connection at this point.
            <vspace blankLines="1"/>
            When doing client ID trunking, locking state is shared across
            sessions associated with that same client ID.  This requires the
            server to coordinate state across sessions.
          </t>
        </list>
      </t>

      <t>
        The client should be prepared for the possibility that
        eir_server_owner values may be different on subsequent EXCHANGE_ID
        requests made to the same network address, as a result of various
        sorts of reconfiguration events.  When this happens and the changes
        result in the invalidation of previously valid forms of trunking, the
        client should cease to use those forms, either by dropping
        connections or by adding sessions.  For a discussion of lock reclaim
        as it relates to such reconfiguration events, see <xref target="ss:state_management:SR" />.
      </t>

      <section toc='exclude' anchor="ss:core_infrastructure:VCoMSI" title="Verifying Claims of Matching Server Identity">

        <t>
          When two servers over two connections claim matching or partially
          matching eir_server_owner, eir_server_scope, and eir_clientid values,
          the client does not have to trust the servers' claims.  The client
          may verify these claims before trunking traffic in the following
          ways:
        </t>

        <t>
          <list style='symbols'>
            <t>
              For session trunking, clients SHOULD reliably verify if
              connections between different network paths are in fact associated
              with the same NFSv4.1 server and usable on the same session, and
              servers MUST allow clients to perform reliable verification.  When
              a client ID is created, the client SHOULD specify that
              BIND_CONN_TO_SESSION is to be verified according to the SP4_SSV or
              SP4_MACH_CRED (<xref target="op_EXCHANGE_ID" />) state protection options.  For
              SP4_SSV, reliable verification depends on a shared secret (the
              SSV) that is established via the SET_SSV (<xref target="op_SET_SSV" />)
              operation.
              <vspace blankLines="1"/>
              When a new connection is associated with the session (via the
              BIND_CONN_TO_SESSION operation, see <xref target="op_BIND_CONN_TO_SESSION" />), if the client
              specified SP4_SSV state protection for the BIND_CONN_TO_SESSION
              operation, the client MUST send the BIND_CONN_TO_SESSION with
              RPCSEC_GSS protection, using integrity or privacy, and an
              RPCSEC_GSS handle created with the GSS SSV mechanism
              (<xref target="ss:core_infrastructure:TSSVGM" />).
              <vspace blankLines="1"/>
              If the client mistakenly tries to associate a connection to a
              session of a wrong server, the server will either reject the
              attempt because it is not aware of the session identifier of the
              BIND_CONN_TO_SESSION arguments, or it will reject the attempt
              because the RPCSEC_GSS authentication fails.  Even if the server
              mistakenly or maliciously accepts the connection association
              attempt, the RPCSEC_GSS verifier it computes in the response will
              not be verified by the client, so the client will know it cannot
              use the connection for trunking the specified session.
              <vspace blankLines="1"/>
              If the client specified SP4_MACH_CRED state protection, the
              BIND_CONN_TO_SESSION operation will use RPCSEC_GSS integrity or
              privacy, using the same credential that was used when the client
              ID was created.  Mutual authentication via RPCSEC_GSS assures the
              client that the connection is associated with the correct session
              of the correct server.
            </t>

            <t>
              For client ID trunking, the client has at least two options for
              verifying that the same client ID obtained from two different
              EXCHANGE_ID operations came from the same server.  The first
              option is to use RPCSEC_GSS authentication when sending each
              EXCHANGE_ID operation.  Each time an EXCHANGE_ID is sent with
              RPCSEC_GSS authentication, the client notes the principal name of
              the GSS target.  If the EXCHANGE_ID results indicate that client
              ID trunking is possible, and the GSS targets' principal names are
              the same, the servers are the same and client ID trunking is
              allowed.
              <vspace blankLines="1"/>
              The second option for verification is to use SP4_SSV protection.
              When the client sends EXCHANGE_ID, it specifies SP4_SSV
              protection.  The first EXCHANGE_ID the client sends always has to
              be confirmed by a CREATE_SESSION call.  The client then sends
              SET_SSV.  Later, the client sends EXCHANGE_ID to a second
              destination network address different from the one the first
              EXCHANGE_ID was sent to.  The client checks that each EXCHANGE_ID
              reply has the same eir_clientid, eir_server_owner.so_major_id, and
              eir_server_scope.  If so, the client verifies the claim by sending
              a CREATE_SESSION operation to the second destination address,
              protected with RPCSEC_GSS integrity using an RPCSEC_GSS handle
              returned by the second EXCHANGE_ID.  If the server accepts the
              CREATE_SESSION request, and if the client verifies the RPCSEC_GSS
              verifier and integrity codes, then the client has proof the second
              server knows the SSV, and thus the two servers are cooperating for
              the purposes of specifying server scope and client ID trunking.
            </t>
          </list>
        </t>
      </section>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:EOS" title="Exactly Once Semantics">
      <t>
        Via the session, NFSv4.1 offers exactly once semantics (EOS) for
        requests sent over a channel.  EOS is supported on both the fore
        channel and backchannel.
      </t>

      <t>
        Each COMPOUND or CB_COMPOUND request that is sent with a leading
        SEQUENCE or CB_SEQUENCE operation MUST be executed by the receiver
        exactly once.  This requirement holds regardless of whether the
        request is sent with reply caching specified (see
        <xref target="ss:core_infrastructure:ORC" />).  The requirement holds even if the requester is
        sending the request over a session created between a pNFS data client
        and pNFS data server.  To understand the rationale for this
        requirement, divide the requests into three classifications:
      </t>

      <t>
        <list style='symbols'>
          <t>
            Non-idempotent requests.
          </t>

          <t>
            Idempotent modifying requests.
          </t>

          <t>
            Idempotent non-modifying requests.
          </t>
        </list>
      </t>

      <t>
        An example of a non-idempotent request is RENAME.  Obviously, if a
        replier executes the same RENAME request twice, and the first
        execution succeeds, the re-execution will fail.  If the replier
        returns the result from the re-execution, this result is incorrect.
        Therefore, EOS is required for non-idempotent requests.
      </t>

      <t>
        An example of an idempotent modifying request is a COMPOUND request
        containing a WRITE operation.  Repeated execution of the same WRITE
        has the same effect as execution of that WRITE a single time.
        Nevertheless, enforcing EOS for WRITEs and other idempotent modifying
        requests is necessary to avoid data corruption.
      </t>

      <t>
        Suppose a client sends WRITE A to a noncompliant server that does not
        enforce EOS, and receives no response, perhaps due to a network
        partition.  The client reconnects to the server and re-sends WRITE A.
        Now, the server has outstanding two instances of A.  The server can
        be in a situation in which it executes and replies to the retry of A,
        while the first A is still waiting in the server's internal I/O
        system for some resource.  Upon receiving the reply to the second
        attempt of WRITE A, the client believes its WRITE is done so it is
        free to send WRITE B, which overlaps the byte-range of A.  When the
        original A is dispatched from the server's I/O system and executed
        (thus the second time A will have been written), then what has been
        written by B can be overwritten and thus corrupted.
      </t>

      <t>
        An example of an idempotent non-modifying request is a COMPOUND
        containing SEQUENCE, PUTFH, READLINK, and nothing else.  The
        re-execution of such a request will not cause data corruption or produce
        an incorrect result.  Nonetheless, to keep the implementation simple,
        the replier MUST enforce EOS for all requests, whether or not
        idempotent and non-modifying.
      </t>

      <t>
        Note that true and complete EOS is not possible unless the server
        persists the reply cache in stable storage, and unless the server is
        somehow implemented to never require a restart (indeed, if such a
        server exists, the distinction between a reply cache kept in stable
        storage versus one that is not is one without meaning).  See
        <xref target="ss:core_infrastructure:P1" /> for a discussion of persistence in the reply cache.
        Regardless, even if the server does not persist the reply cache, EOS
        improves robustness and correctness over previous versions of NFS
        because the legacy duplicate request/reply caches were based on the
        ONC RPC transaction identifier (XID).  <xref target="ss:core_infrastructure:SIaRC" /> explains the
        shortcomings of the XID as a basis for a reply cache and describes
        how NFSv4.1 sessions improve upon the XID.
      </t>

      <section toc='exclude' anchor="ss:core_infrastructure:SIaRC" title="Slot Identifiers and Reply Cache">
        <t>
          The RPC layer provides a transaction ID (XID), which, while required
          to be unique, is not convenient for tracking requests for two
          reasons.  First, the XID is only meaningful to the requester; it
          cannot be interpreted by the replier except to test for equality with
          previously sent requests.  When consulting an RPC-based duplicate
          request cache, the opaqueness of the XID requires a computationally
          expensive look up (often via a hash that includes XID and source
          address).  NFSv4.1 requests use a non-opaque slot ID, which is an
          index into a slot table, which is far more efficient.  Second,
          because RPC requests can be executed by the replier in any order,
          there is no bound on the number of requests that may be outstanding
          at any time.  To achieve perfect EOS, using ONC RPC would require
          storing all replies in the reply cache.  XIDs are 32 bits; storing
          over four billion (2^32) replies in the reply cache is not practical.
          In practice, previous versions of NFS have chosen to store a fixed
          number of replies in the cache, and to use a least recently used
          (LRU) approach to replacing cache entries with new entries when the
          cache is full.  In NFSv4.1, the number of outstanding requests is
          bounded by the size of the slot table, and a sequence ID per slot is
          used to tell the replier when it is safe to delete a cached reply.
        </t>

        <t>
          In the NFSv4.1 reply cache, when the requester sends a new request,
          it selects a slot ID in the range 0..N, where N is the replier's
          current maximum slot ID granted to the requester on the session over
          which the request is to be sent.  The value of N starts out as equal
          to ca_maxrequests - 1 (<xref target="op_CREATE_SESSION" />), but can be adjusted by the
          response to SEQUENCE or CB_SEQUENCE as described later in this
          section.  The slot ID must be unused by any of the requests that the
          requester has already active on the session.  "Unused" here means the
          requester has no outstanding request for that slot ID.
        </t>

        <t>
          A slot contains a sequence ID and the cached reply corresponding to
          the request sent with that sequence ID.  The sequence ID is a 32-bit
          unsigned value, and is therefore in the range 0..0xFFFFFFFF (2^32 -
          1).  The first time a slot is used, the requester MUST specify a
          sequence ID of one (<xref target="op_CREATE_SESSION" />).  Each time a slot is reused, the
          request MUST specify a sequence ID that is one greater than that of
          the previous request on the slot.  If the previous sequence ID was
          0xFFFFFFFF, then the next request for the slot MUST have the sequence
          ID set to zero (i.e., (2^32 - 1) + 1 mod 2^32).
        </t>

        <t>
          The sequence ID accompanies the slot ID in each request.  It is for
          the critical check at the replier: it used to efficiently determine
          whether a request using a certain slot ID is a retransmit or a new,
          never-before-seen request.  It is not feasible for the requester to
          assert that it is retransmitting to implement this, because for any
          given request the requester cannot know whether the replier has seen
          it unless the replier actually replies.  Of course, if the requester
          has seen the reply, the requester would not retransmit.
        </t>

        <t>
          The replier compares each received request's sequence ID with the
          last one previously received for that slot ID, to see if the new
          request is:
        </t>

        <t>
          <list style='symbols'>
            <t>
              A new request, in which the sequence ID is one greater than that
              previously seen in the slot (accounting for sequence wraparound).
              The replier proceeds to execute the new request, and the replier
              MUST increase the slot's sequence ID by one.
            </t>

            <t>
              A retransmitted request, in which the sequence ID is equal to that
              currently recorded in the slot.  If the original request has
              executed to completion, the replier returns the cached reply.  See
              <xref target="ss:core_infrastructure:RaRoR" /> for direction on how the replier deals with
              retries of requests that are still in progress.
            </t>

            <t>
              A misordered retry, in which the sequence ID is less than
              (accounting for sequence wraparound) that previously seen in the
              slot.  The replier MUST return NFS4ERR_SEQ_MISORDERED (as the
              result from SEQUENCE or CB_SEQUENCE).
            </t>

            <t>
              A misordered new request, in which the sequence ID is two or more
              than (accounting for sequence wraparound) that previously seen in
              the slot.  Note that because the sequence ID MUST wrap around to
              zero once it reaches 0xFFFFFFFF, a misordered new request and a
              misordered retry cannot be distinguished.  Thus, the replier MUST
              return NFS4ERR_SEQ_MISORDERED (as the result from SEQUENCE or
              CB_SEQUENCE).
            </t>
          </list>
        </t>

        <t>
          Unlike the XID, the slot ID is always within a specific range; this
          has two implications.  The first implication is that for a given
          session, the replier need only cache the results of a limited number
          of COMPOUND requests.  The second implication derives from the first,
          which is that unlike XID-indexed reply caches (also known as
          duplicate request caches - DRCs), the slot ID-based reply cache
          cannot be overflowed.  Through use of the sequence ID to identify
          retransmitted requests, the replier does not need to actually cache
          the request itself, reducing the storage requirements of the reply
          cache further.  These facilities make it practical to maintain all
          the required entries for an effective reply cache.
        </t>

        <t>
          The slot ID, sequence ID, and session ID therefore take over the
          traditional role of the XID and source network address in the
          replier's reply cache implementation.  This approach is considerably
          more portable and completely robust -- it is not subject to the
          reassignment of ports as clients reconnect over IP networks.  In
          addition, the RPC XID is not used in the reply cache, enhancing
          robustness of the cache in the face of any rapid reuse of XIDs by the
          requester.  While the replier does not care about the XID for the
          purposes of reply cache management (but the replier MUST return the
          same XID that was in the request), nonetheless there are
          considerations for the XID in NFSv4.1 that are the same as all other
          previous versions of NFS.  The RPC XID remains in each message and
          needs to be formulated in NFSv4.1 requests as in any other ONC RPC
          request.  The reasons include:
        </t>

        <t>
          <list style='symbols'>
            <t>
              The RPC layer retains its existing semantics and implementation.
            </t>

            <t>
              The requester and replier must be able to interoperate at the RPC
              layer, prior to the NFSv4.1 decoding of the SEQUENCE or
              CB_SEQUENCE operation.
            </t>

            <t>
              If an operation is being used that does not start with SEQUENCE or
              CB_SEQUENCE (e.g., BIND_CONN_TO_SESSION), then the RPC XID is
              needed for correct operation to match the reply to the request.
            </t>

            <t>
              The SEQUENCE or CB_SEQUENCE operation may generate an error.  If
              so, the embedded slot ID, sequence ID, and session ID (if present)
              in the request will not be in the reply, and the requester has
              only the XID to match the reply to the request.
            </t>
          </list>
        </t>

        <t>
          Given that well-formulated XIDs continue to be required, this begs
          the question: why do SEQUENCE and CB_SEQUENCE replies have a session
          ID, slot ID, and sequence ID?  Having the session ID in the reply
          means that the requester does not have to use the XID to look up the
          session ID, which would be necessary if the connection were
          associated with multiple sessions.  Having the slot ID and sequence
          ID in the reply means that the requester does not have to use the XID
          to look up the slot ID and sequence ID.  Furthermore, since the XID
          is only 32 bits, it is too small to guarantee the re-association of a
          reply with its request <xref target="rpc_xid_issues" />; having session ID, slot ID, and sequence
          ID in the reply allows the client to validate that the reply in fact
          belongs to the matched request.
        </t>

        <t>
          The SEQUENCE (and CB_SEQUENCE) operation also carries a
          "highest_slotid" value, which carries additional requester slot usage
          information.  The requester MUST always indicate the slot ID
          representing the outstanding request with the highest-numbered slot
          value.  The requester should in all cases provide the most
          conservative value possible, although it can be increased somewhat
          above the actual instantaneous usage to maintain some minimum or
          optimal level.  This provides a way for the requester to yield unused
          request slots back to the replier, which in turn can use the
          information to reallocate resources.
        </t>

        <t>
          The replier responds with both a new target highest_slotid and an
          enforced highest_slotid, described as follows:
        </t>

        <t>
          <list style='symbols'>
            <t>
              The target highest_slotid is an indication to the requester of the
              highest_slotid the replier wishes the requester to be using.  This
              permits the replier to withdraw (or add) resources from a
              requester that has been found to not be using them, in order to
              more fairly share resources among a varying level of demand from
              other requesters.  The requester must always comply with the
              replier's value updates, since they indicate newly established
              hard limits on the requester's access to session resources.
              However, because of request pipelining, the requester may have
              active requests in flight reflecting prior values; therefore, the
              replier must not immediately require the requester to comply.
            </t>

            <t>
              The enforced highest_slotid indicates the highest slot ID the
              requester is permitted to use on a subsequent SEQUENCE or
              CB_SEQUENCE operation.  The replier's enforced highest_slotid
              SHOULD be no less than the highest_slotid the requester indicated
              in the SEQUENCE or CB_SEQUENCE arguments.
              <vspace blankLines="1"/>
              A requester can be intransigent with respect to lowering its
              highest_slotid argument to a Sequence operation, i.e. the
              requester continues to ignore the target highest_slotid in the
              response to a Sequence operation, and continues to set its
              highest_slotid argument to be higher than the target
              highest_slotid.  This can be considered particularly egregious
              behavior when the replier knows there are no outstanding requests
              with slot IDs higher than its target highest_slotid.  When faced
              with such intransigence, the replier is free to take more forceful
              action, and MAY reply with a new enforced highest_slotid that is
              less than its previous enforced highest_slotid.  Thereafter, if
              the requester continues to send requests with a highest_slotid
              that is greater than the replier's new enforced highest_slotid,
              the server MAY return NFS4ERR_BAD_HIGH_SLOT, unless the slot ID in
              the request is greater than the new enforced highest_slotid and
              the request is a retry.
              <vspace blankLines="1"/>
              The replier SHOULD retain the slots it wants to retire until the
              requester sends a request with a highest_slotid less than or equal
              to the replier's new enforced highest_slotid.
              <vspace blankLines="1"/>
              The requester can also be intransigent with respect to sending
              non-retry requests that have a slot ID that exceeds the replier's
              highest_slotid.  Once the replier has forcibly lowered the
              enforced highest_slotid, the requester is only allowed to send
              retries on slots that exceed the replier's highest_slotid.  If a
              request is received with a slot ID that is higher than the new
              enforced highest_slotid, and the sequence ID is one higher than
              what is in the slot's reply cache, then the server can both retire
              the slot and return NFS4ERR_BADSLOT (however, the server MUST NOT
              do one and not the other).  The reason it is safe to retire the
              slot is because by using the next sequence ID, the requester is
              indicating it has received the previous reply for the slot.
            </t>

            <t>
              The requester SHOULD use the lowest available slot when sending a
              new request.  This way, the replier may be able to retire slot
              entries faster.  However, where the replier is actively adjusting
              its granted highest_slotid, it will not be able to use only the
              receipt of the slot ID and highest_slotid in the request.  Neither
              the slot ID nor the highest_slotid used in a request may reflect
              the replier's current idea of the requester's session limit,
              because the request may have been sent from the requester before
              the update was received.  Therefore, in the downward adjustment
              case, the replier may have to retain a number of reply cache
              entries at least as large as the old value of maximum requests
              outstanding, until it can infer that the requester has seen a
              reply containing the new granted highest_slotid.  The replier can
              infer that the requester has seen such a reply when it receives a
              new request with the same slot ID as the request replied to and
              the next higher sequence ID.
            </t>
          </list>
        </t>

        <section toc='exclude' anchor="ss:core_infrastructure:CoSaCR" title="Caching of SEQUENCE and CB_SEQUENCE Replies">
          <t>
            When a SEQUENCE or CB_SEQUENCE operation is successfully executed,
            its reply MUST always be cached.  Specifically, session ID, sequence
            ID, and slot ID MUST be cached in the reply cache.  The reply from
            SEQUENCE also includes the highest slot ID, target highest slot ID,
            and status flags.  Instead of caching these values, the server MAY
            re-compute the values from the current state of the fore channel,
            session, and/or client ID as appropriate.  Similarly, the reply from
            CB_SEQUENCE includes a highest slot ID and target highest slot ID.
            The client MAY re-compute the values from the current state of the
            session as appropriate.
          </t>

          <t>
            Regardless of whether or not a replier is re-computing highest slot
            ID, target slot ID, and status on replies to retries, the requester
            MUST NOT assume that the values are being re-computed whenever it
            receives a reply after a retry is sent, since it has no way of
            knowing whether the reply it has received was sent by the replier in
            response to the retry or is a delayed response to the original
            request.  Therefore, it may be the case that highest slot ID, target
            slot ID, or status bits may reflect the state of affairs when the
            request was first executed.  Although acting based on such delayed
            information is valid, it may cause the receiver of the reply to do
            unneeded work.  Requesters MAY choose to send additional requests to
            get the current state of affairs or use the state of affairs reported
            by subsequent requests, in preference to acting immediately on data
            that might be out of date.
          </t>
        </section>

        <section toc='exclude' anchor="ss:core_infrastructure:EfSaC" title="Errors from SEQUENCE and CB_SEQUENCE">
          <t>
            Any time SEQUENCE or CB_SEQUENCE returns an error, the sequence ID of
            the slot MUST NOT change.  The replier MUST NOT modify the reply
            cache entry for the slot whenever an error is returned from SEQUENCE
            or CB_SEQUENCE.
          </t>
        </section>

        <section toc='exclude' anchor="ss:core_infrastructure:ORC" title="Optional Reply Caching">
          <t>
            On a per-request basis, the requester can choose to direct the
            replier to cache the reply to all operations after the first
            operation (SEQUENCE or CB_SEQUENCE) via the sa_cachethis or
            csa_cachethis fields of the arguments to SEQUENCE or CB_SEQUENCE.
            The reason it would not direct the replier to cache the entire reply
            is that the request is composed of all idempotent operations <xref target="Chet" />.
            Caching the reply may offer little benefit.  If the reply is too
            large (see <xref target="ss:core_infrastructure:CaCCI" />), it may not be cacheable anyway.  Even
            if the reply to idempotent request is small enough to cache,
            unnecessarily caching the reply slows down the server and increases
            RPC latency.
          </t>

          <t>
            Whether or not the requester requests the reply to be cached has no
            effect on the slot processing.  If the results of SEQUENCE or
            CB_SEQUENCE are NFS4_OK, then the slot's sequence ID MUST be
            incremented by one.  If a requester does not direct the replier to
            cache the reply, the replier MUST do one of following:
          </t>

          <t>
            <list style='symbols'>
              <t>
                The replier can cache the entire original reply.  Even though
                sa_cachethis or csa_cachethis is FALSE, the replier is always free
                to cache.  It may choose this approach in order to simplify
                implementation.
              </t>

              <t>
                The replier enters into its reply cache a reply consisting of the
                original results to the SEQUENCE or CB_SEQUENCE operation, and
                with the next operation in COMPOUND or CB_COMPOUND having the
                error NFS4ERR_RETRY_UNCACHED_REP.  Thus, if the requester later
                retries the request, it will get NFS4ERR_RETRY_UNCACHED_REP.  If a
                replier receives a retried Sequence operation where the reply to
                the COMPOUND or CB_COMPOUND was not cached, then the replier,
                <vspace blankLines="1"/>
                <list style='symbols'>
                  <t>
                    MAY return NFS4ERR_RETRY_UNCACHED_REP in reply to a Sequence
                    operation if the Sequence operation is not the first operation
                    (granted, a requester that does so is in violation of the
                    NFSv4.1 protocol).
                  </t>

                  <t>
                    MUST NOT return NFS4ERR_RETRY_UNCACHED_REP in reply to a
                    Sequence operation if the Sequence operation is the first
                    operation.
                  </t>
                </list>
              </t>

              <t>
                If the second operation is an illegal operation, or an operation
                that was legal in a previous minor version of NFSv4 and MUST NOT
                be supported in the current minor version (e.g., SETCLIENTID), the
                replier MUST NOT ever return NFS4ERR_RETRY_UNCACHED_REP.  Instead
                the replier MUST return NFS4ERR_OP_ILLEGAL or NFS4ERR_BADXDR or
                NFS4ERR_NOTSUPP as appropriate.
              </t>

              <t>
                If the second operation can result in another error status, the
                replier MAY return a status other than NFS4ERR_RETRY_UNCACHED_REP,
                provided the operation is not executed in such a way that the
                state of the replier is changed.  Examples of such an error status
                include: NFS4ERR_NOTSUPP returned for an operation that is legal
                but not REQUIRED in the current minor versions, and thus not
                supported by the replier; NFS4ERR_SEQUENCE_POS; and
                NFS4ERR_REQ_TOO_BIG.
              </t>
            </list>
          </t>

          <t>
            The discussion above assumes that the retried request matches the
            original one.  <xref target="ss:core_infrastructure:FR" /> discusses what the replier might
            do, and MUST do when original and retried requests do not match.
            Since the replier may only cache a small amount of the information
            that would be required to determine whether this is a case of a false
            retry, the replier may send to the client any of the following
            responses:
          </t>

          <t>
            <list style='symbols'>
              <t>
                The cached reply to the original request (if the replier has
                cached it in its entirety and the users of the original request
                and retry match).
              </t>

              <t>
                A reply that consists only of the Sequence operation with the
                error NFS4ERR_SEQ_FALSE_RETRY.
              </t>

              <t>
                A reply consisting of the response to Sequence with the status
                NFS4_OK, together with the second operation as it appeared in the
                retried request with an error of NFS4ERR_RETRY_UNCACHED_REP or
                other error as described above.
              </t>

              <t>
                A reply that consists of the response to Sequence with the status
                NFS4_OK, together with the second operation as it appeared in the
                original request with an error of NFS4ERR_RETRY_UNCACHED_REP or
                other error as described above.
              </t>
            </list>
          </t>

          <section toc='exclude' anchor="ss:core_infrastructure:FR" title="False Retry">
            <t>
              If a requester sent a Sequence operation with a slot ID and sequence
              ID that are in the reply cache but the replier detected that the
              retried request is not the same as the original request, including a
              retry that has different operations or different arguments in the
              operations from the original and a retry that uses a different
              principal in the RPC request's credential field that translates to a
              different user, then this is a false retry.  When the replier detects
              a false retry, it is permitted (but not always obligated) to return
              NFS4ERR_SEQ_FALSE_RETRY in response to the Sequence operation when it
              detects a false retry.
            </t>

            <t>
              Translations of particularly privileged user values to other users
              due to the lack of appropriately secure credentials, as configured on
              the replier, should be applied before determining whether the users
              are the same or different.  If the replier determines the users are
              different between the original request and a retry, then the replier
              MUST return NFS4ERR_SEQ_FALSE_RETRY.
            </t>

            <t>
              If an operation of the retry is an illegal operation, or an operation
              that was legal in a previous minor version of NFSv4 and MUST NOT be
              supported in the current minor version (e.g., SETCLIENTID), the
              replier MAY return NFS4ERR_SEQ_FALSE_RETRY (and MUST do so if the users
              of the original request and retry differ).  Otherwise, the replier
              MAY return NFS4ERR_OP_ILLEGAL or NFS4ERR_BADXDR or NFS4ERR_NOTSUPP as
              appropriate.  Note that the handling is in contrast for how the
              replier deals with retries requests with no cached reply.  The
              difference is due to NFS4ERR_SEQ_FALSE_RETRY being a valid error for only
              Sequence operations, whereas NFS4ERR_RETRY_UNCACHED_REP is a valid
              error for all operations except illegal operations and operations
              that MUST NOT be supported in the current minor version of NFSv4.
            </t>
          </section>
        </section>
      </section>

      <section toc='exclude' anchor="ss:core_infrastructure:RaRoR" title="Retry and Replay of Reply">
        <t>
          A requester MUST NOT retry a request, unless the connection it used
          to send the request disconnects.  The requester can then reconnect
          and re-send the request, or it can re-send the request over a
          different connection that is associated with the same session.
        </t>

        <t>
          If the requester is a server wanting to re-send a callback operation
          over the backchannel of a session, the requester of course cannot
          reconnect because only the client can associate connections with the
          backchannel.  The server can re-send the request over another
          connection that is bound to the same session's backchannel.  If there
          is no such connection, the server MUST indicate that the session has
          no backchannel by setting the SEQ4_STATUS_CB_PATH_DOWN_SESSION flag
          bit in the response to the next SEQUENCE operation from the client.
          The client MUST then associate a connection with the session (or
          destroy the session).
        </t>

        <t>
          Note that it is not fatal for a requester to retry without a
          disconnect between the request and retry.  However, the retry does
          consume resources, especially with RDMA, where each request, retry or
          not, consumes a credit.  Retries for no reason, especially retries
          sent shortly after the previous attempt, are a poor use of network
          bandwidth and defeat the purpose of a transport's inherent congestion
          control system.
        </t>

        <t>
          A requester MUST wait for a reply to a request before using the slot
          for another request.  If it does not wait for a reply, then the
          requester does not know what sequence ID to use for the slot on its
          next request.  For example, suppose a requester sends a request with
          sequence ID 1, and does not wait for the response.  The next time it
          uses the slot, it sends the new request with sequence ID 2.  If the
          replier has not seen the request with sequence ID 1, then the replier
          is not expecting sequence ID 2, and rejects the requester's new
          request with NFS4ERR_SEQ_MISORDERED (as the result from SEQUENCE or
          CB_SEQUENCE).
        </t>

        <t>
          RDMA fabrics do not guarantee that the memory handles (Steering Tags)
          within each RPC/RDMA "chunk" <xref target="RFC5666" /> are valid on a scope outside that of
          a single connection.  Therefore, handles used by the direct
          operations become invalid after connection loss.  The server must
          ensure that any RDMA operations that must be replayed from the reply
          cache use the newly provided handle(s) from the most recent request.
        </t>

        <t>
          A retry might be sent while the original request is still in progress
          on the replier.  The replier SHOULD deal with the issue by returning
          NFS4ERR_DELAY as the reply to a SEQUENCE or CB_SEQUENCE operation, but
          implementations MAY return NFS4ERR_SEQ_MISORDERED.  Since errors from
          SEQUENCE and CB_SEQUENCE are never recorded in the reply cache, this
          approach allows the results of the execution of the original request
          to be properly recorded in the reply cache (assuming that the
          requester specified the reply to be cached).
        </t>
      </section>

      <section toc='exclude' anchor="ss:core_infrastructure:RSCR" title="Resolving Server Callback Races">
        <t>
          It is possible for server callbacks to arrive at the client before
          the reply from related fore channel operations.  For example, a
          client may have been granted a delegation to a file it has opened,
          but the reply to the OPEN (informing the client of the granting of
          the delegation) may be delayed in the network.  If a conflicting
          operation arrives at the server, it will recall the delegation using
          the backchannel, which may be on a different transport connection,
          perhaps even a different network, or even a different session
          associated with the same client ID.
        </t>

        <t>
          The presence of a session between the client and server alleviates
          this issue.  When a session is in place, each client request is
          uniquely identified by its { session ID, slot ID, sequence ID }
          triple.  By the rules under which slot entries (reply cache entries)
          are retired, the server has knowledge whether the client has "seen"
          each of the server's replies.  The server can therefore provide
          sufficient information to the client to allow it to disambiguate
          between an erroneous or conflicting callback race condition.
        </t>

        <t>
          For each client operation that might result in some sort of server
          callback, the server SHOULD "remember" the { session ID, slot ID,
          sequence ID } triple of the client request until the slot ID
          retirement rules allow the server to determine that the client has,
          in fact, seen the server's reply.  Until the time the { session ID,
          slot ID, sequence ID } request triple can be retired, any recalls of
          the associated object MUST carry an array of these referring
          identifiers (in the CB_SEQUENCE operation's arguments), for the
          benefit of the client.  After this time, it is not necessary for the
          server to provide this information in related callbacks, since it is
          certain that a race condition can no longer occur.
        </t>

        <t>
          The CB_SEQUENCE operation that begins each server callback carries a
          list of "referring" { session ID, slot ID, sequence ID } triples.  If
          the client finds the request corresponding to the referring session
          ID, slot ID, and sequence ID to be currently outstanding (i.e., the
          server's reply has not been seen by the client), it can determine
          that the callback has raced the reply, and act accordingly.  If the
          client does not find the request corresponding to the referring
          triple to be outstanding (including the case of a session ID
          referring to a destroyed session), then there is no race with respect
          to this triple.  The server SHOULD limit the referring triples to
          requests that refer to just those that apply to the objects referred
          to in the CB_COMPOUND procedure.
        </t>

        <t>
          The client must not simply wait forever for the expected server reply
          to arrive before responding to the CB_COMPOUND that won the race,
          because it is possible that it will be delayed indefinitely.  The
          client should assume the likely case that the reply will arrive
          within the average round-trip time for COMPOUND requests to the
          server, and wait that period of time.  If that period of time
          expires, it can respond to the CB_COMPOUND with NFS4ERR_DELAY.  There
          are other scenarios under which callbacks may race replies.  Among
          them are pNFS layout recalls as described in <xref target="ss:parallel_nfs:SoLO" />.
        </t>
      </section>

      <section toc='exclude' anchor="ss:core_infrastructure:CaCCI" title="COMPOUND and CB_COMPOUND Construction Issues">
        <t>
          Very large requests and replies may pose both buffer management
          issues (especially with RDMA) and reply cache issues.  When the
          session is created (<xref target="op_CREATE_SESSION" />), for each channel (fore and back),
          the client and server negotiate the maximum-sized request they will
          send or process (ca_maxrequestsize), the maximum-sized reply they
          will return or process (ca_maxresponsesize), and the maximum-sized
          reply they will store in the reply cache (ca_maxresponsesize_cached).
        </t>

        <t>
          If a request exceeds ca_maxrequestsize, the reply will have the
          status NFS4ERR_REQ_TOO_BIG.  A replier MAY return NFS4ERR_REQ_TOO_BIG
          as the status for the first operation (SEQUENCE or CB_SEQUENCE) in
          the request (which means that no operations in the request executed
          and that the state of the slot in the reply cache is unchanged), or
          it MAY opt to return it on a subsequent operation in the same
          COMPOUND or CB_COMPOUND request (which means that at least one
          operation did execute and that the state of the slot in the reply
          cache does change).  The replier SHOULD set NFS4ERR_REQ_TOO_BIG on
          the operation that exceeds ca_maxrequestsize.
        </t>

        <t>
          If a reply exceeds ca_maxresponsesize, the reply will have the status
          NFS4ERR_REP_TOO_BIG.  A replier MAY return NFS4ERR_REP_TOO_BIG as the
          status for the first operation (SEQUENCE or CB_SEQUENCE) in the
          request, or it MAY opt to return it on a subsequent operation (in the
          same COMPOUND or CB_COMPOUND reply).  A replier MAY return
          NFS4ERR_REP_TOO_BIG in the reply to SEQUENCE or CB_SEQUENCE, even if
          the response would still exceed ca_maxresponsesize.
        </t>

        <t>
          If sa_cachethis or csa_cachethis is TRUE, then the replier MUST cache
          a reply except if an error is returned by the SEQUENCE or CB_SEQUENCE
          operation (see <xref target="ss:core_infrastructure:EfSaC" />).  If the reply exceeds
          ca_maxresponsesize_cached (and sa_cachethis or csa_cachethis is
          TRUE), then the server MUST return NFS4ERR_REP_TOO_BIG_TO_CACHE.
          Even if NFS4ERR_REP_TOO_BIG_TO_CACHE (or any other error for that
          matter) is returned on an operation other than the first operation
          (SEQUENCE or CB_SEQUENCE), then the reply MUST be cached if
          sa_cachethis or csa_cachethis is TRUE.  For example, if a COMPOUND
          has eleven operations, including SEQUENCE, the fifth operation is a
          RENAME, and the tenth operation is a READ for one million bytes, the
          server may return NFS4ERR_REP_TOO_BIG_TO_CACHE on the tenth
          operation.  Since the server executed several operations, especially
          the non-idempotent RENAME, the client's request to cache the reply
          needs to be honored in order for the correct operation of exactly
          once semantics.  If the client retries the request, the server will
          have cached a reply that contains results for ten of the eleven
          requested operations, with the tenth operation having a status of
          NFS4ERR_REP_TOO_BIG_TO_CACHE.
        </t>

        <t>
          A client needs to take care that when sending operations that change
          the current filehandle (except for PUTFH, PUTPUBFH, PUTROOTFH, and
          RESTOREFH), it not exceed the maximum reply buffer before the GETFH
          operation.  Otherwise, the client will have to retry the operation
          that changed the current filehandle, in order to obtain the desired
          filehandle.  For the OPEN operation (see <xref target="op_OPEN" />), retry is not
          always available as an option.  The following guidelines for the
          handling of filehandle-changing operations are advised:
        </t>

        <t>
          <list style='symbols'>
            <t>
              Within the same COMPOUND procedure, a client SHOULD send GETFH
              immediately after a current filehandle-changing operation.  A
              client MUST send GETFH after a current filehandle-changing
              operation that is also non-idempotent (e.g., the OPEN operation),
              unless the operation is RESTOREFH.  RESTOREFH is an exception,
              because even though it is non-idempotent, the filehandle RESTOREFH
              produced originated from an operation that is either idempotent
              (e.g., PUTFH, LOOKUP), or non-idempotent (e.g., OPEN, CREATE).  If
              the origin is non-idempotent, then because the client MUST send
              GETFH after the origin operation, the client can recover if
              RESTOREFH returns an error.
            </t>

            <t>
              A server MAY return NFS4ERR_REP_TOO_BIG or
              NFS4ERR_REP_TOO_BIG_TO_CACHE (if sa_cachethis is TRUE) on a
              filehandle-changing operation if the reply would be too large on
              the next operation.
            </t>

            <t>
              A server SHOULD return NFS4ERR_REP_TOO_BIG or
              NFS4ERR_REP_TOO_BIG_TO_CACHE (if sa_cachethis is TRUE) on a
              filehandle-changing, non-idempotent operation if the reply would
              be too large on the next operation, especially if the operation is
              OPEN.
            </t>

            <t>
              A server MAY return NFS4ERR_UNSAFE_COMPOUND to a non-idempotent
              current filehandle-changing operation, if it looks at the next
              operation (in the same COMPOUND procedure) and finds it is not
              GETFH.  The server SHOULD do this if it is unable to determine in
              advance whether the total response size would exceed
              ca_maxresponsesize_cached or ca_maxresponsesize.
            </t>
          </list>
        </t>
      </section>

      <section toc='exclude' anchor="ss:core_infrastructure:P1" title="Persistence">
        <t>
          Since the reply cache is bounded, it is practical for the reply cache
          to persist across server restarts.  The replier MUST persist the
          following information if it agreed to persist the session (when the
          session was created; see <xref target="op_CREATE_SESSION" />):
        </t>
  
        <t>
          <list style='symbols'>
            <t>
              The session ID.
            </t>
  
            <t>
              The slot table including the sequence ID and cached reply for each
              slot.
            </t>
          </list>
        </t>
  
        <t>
          The above are sufficient for a replier to provide EOS semantics for
          any requests that were sent and executed before the server restarted.
          If the replier is a client, then there is no need for it to persist
          any more information, unless the client will be persisting all other
          state across client restart, in which case, the server will never see
          any NFSv4.1-level protocol manifestation of a client restart.  If the
          replier is a server, with just the slot table and session ID
          persisting, any requests the client retries after the server restart
          will return the results that are cached in the reply cache, and any
          new requests (i.e., the sequence ID is one greater than the slot's
          sequence ID) MUST be rejected with NFS4ERR_DEADSESSION (returned by
          SEQUENCE).  Such a session is considered dead.  A server MAY
          reanimate a session after a server restart so that the session will
          accept new requests as well as retries.  To re-animate a session, the
          server needs to persist additional information through server
          restart:
        </t>
  
        <t>
          <list style='symbols'>
            <t>
              The client ID.  This is a prerequisite to let the client create
              more sessions associated with the same client ID as the reanimated session.
            </t>
  
            <t>
              The client ID's sequence ID that is used for creating sessions
              (see Sections <xref target="op_EXCHANGE_ID" format='counter' /> and <xref target="op_CREATE_SESSION" format='counter' />).  This is a prerequisite to let the
              client create more sessions.
            </t>
  
            <t>
              The principal that created the client ID.  This allows the server
              to authenticate the client when it sends EXCHANGE_ID.
            </t>
  
            <t>
              The SSV, if SP4_SSV state protection was specified when the client
              ID was created (see <xref target="op_EXCHANGE_ID" />).  This lets the client create
              new sessions, and associate connections with the new and existing
              sessions.
            </t>
  
            <t>
              The properties of the client ID as defined in <xref target="op_EXCHANGE_ID" />.
            </t>
          </list>
        </t>
  
        <t>
          A persistent reply cache places certain demands on the server.  The
          execution of the sequence of operations (starting with SEQUENCE) and
          placement of its results in the persistent cache MUST be atomic.  If
          a client retries a sequence of operations that was previously
          executed on the server, the only acceptable outcomes are either the
          original cached reply or an indication that the client ID or session
          has been lost (indicating a catastrophic loss of the reply cache or a
          session that has been deleted because the client failed to use the
          session for an extended period of time).
        </t>
  
        <t>
          A server could fail and restart in the middle of a COMPOUND procedure
          that contains one or more non-idempotent or idempotent-but-modifying
          operations.  This creates an even higher challenge for atomic
          execution and placement of results in the reply cache.  One way to
          view the problem is as a single transaction consisting of each
          operation in the COMPOUND followed by storing the result in
          persistent storage, then finally a transaction commit.  If there is a
          failure before the transaction is committed, then the server rolls
          back the transaction.  If the server itself fails, then when it
          restarts, its recovery logic could roll back the transaction before
          starting the NFSv4.1 server.
        </t>
  
        <t>
          While the description of the implementation for atomic execution of
          the request and caching of the reply is beyond the scope of this
          document, an example implementation for NFSv2 <xref target="RFC1094" /> is described in
          <xref target="ha_nfs_ibm" />.
        </t>
      </section>
    </section>
  
    <section toc='exclude' anchor="ss:core_infrastructure:RC" title="RDMA Considerations">
      <t>
        A complete discussion of the operation of RPC-based protocols over
        RDMA transports is in <xref target="RFC5666" />.  A discussion of the operation of NFSv4,
        including NFSv4.1, over RDMA is in <xref target="RFC5667" />.  Where RDMA is considered,
        this specification assumes the use of such a layering; it addresses
        only the upper-layer issues relevant to making best use of RPC/RDMA.
      </t>
  
      <section toc='exclude' anchor="ss:core_infrastructure:RCR" title="RDMA Connection Resources">
        <t>
          RDMA requires its consumers to register memory and post buffers of a
          specific size and number for receive operations.
        </t>
  
        <t>
          Registration of memory can be a relatively high-overhead operation,
          since it requires pinning of buffers, assignment of attributes (e.g.,
          readable/writable), and initialization of hardware translation.
          Preregistration is desirable to reduce overhead.  These registrations
          are specific to hardware interfaces and even to RDMA connection
          endpoints; therefore, negotiation of their limits is desirable to
          manage resources effectively.
        </t>
  
        <t>
          Following basic registration, these buffers must be posted by the RPC
          layer to handle receives.  These buffers remain in use by the RPC/
          NFSv4.1 implementation; the size and number of them must be known to
          the remote peer in order to avoid RDMA errors that would cause a
          fatal error on the RDMA connection.
        </t>
  
        <t>
          NFSv4.1 manages slots as resources on a per-session basis (see
          <xref target="ss:core_infrastructure:S" />), while RDMA connections
          manage credits on a per-connection basis.  This means that in order for a peer to send data
          over RDMA to a remote buffer, it has to have both an NFSv4.1 slot and
          an RDMA credit.  If multiple RDMA connections are associated with a
          session, then if the total number of credits across all RDMA
          connections associated with the session is X, and the number of slots
          in the session is Y, then the maximum number of outstanding requests
          is the lesser of X and Y.
        </t>
      </section>
  
      <section toc='exclude' anchor="ss:core_infrastructure:FC" title="Flow Control">
        <t>
          Previous versions of NFS do not provide flow control; instead, they
          rely on the windowing provided by transports like TCP to throttle
          requests.  This does not work with RDMA, which provides no operation
          flow control and will terminate a connection in error when limits are
          exceeded.  Limits such as maximum number of requests outstanding are
          therefore negotiated when a session is created (see the
          ca_maxrequests field in <xref target="op_CREATE_SESSION" />).  These limits then provide
          the maxima within which each connection associated with the session's
          channel(s) must remain.  RDMA connections are managed within these
          limits as described in Section 3.3 of <xref target="RFC5666" />; if there are multiple RDMA
          connections, then the maximum number of requests for a channel will
          be divided among the RDMA connections.  Put a different way, the onus
          is on the replier to ensure that the total number of RDMA credits
          across all connections associated with the replier's channel does
          exceed the channel's maximum number of outstanding requests.
        </t>
  
        <t>
          The limits may also be modified dynamically at the replier's choosing
          by manipulating certain parameters present in each NFSv4.1 reply.  In
          addition, the CB_RECALL_SLOT callback operation (see <xref target="op_CB_RECALL_SLOT" />)
          can be sent by a server to a client to return RDMA credits to the
          server, thereby lowering the maximum number of requests a client can
          have outstanding to the server.
        </t>
      </section>
  
      <section toc='exclude' anchor="ss:core_infrastructure:P2" title="Padding">
        <t>
          Header padding is requested by each peer at session initiation (see
          the ca_headerpadsize argument to CREATE_SESSION in <xref target="op_CREATE_SESSION" />),
          and subsequently used by the RPC RDMA layer, as described in <xref target="RFC5666" />.
          Zero padding is permitted.
        </t>
  
        <t>
          Padding leverages the useful property that RDMA preserve alignment of
          data, even when they are placed into anonymous (untagged) buffers.
          If requested, client inline writes will insert appropriate pad bytes
          within the request header to align the data payload on the specified
          boundary.  The client is encouraged to add sufficient padding (up to
          the negotiated size) so that the "data" field of the WRITE operation
          is aligned.  Most servers can make good use of such padding, which
          allows them to chain receive buffers in such a way that any data
          carried by client requests will be placed into appropriate buffers at
          the server, ready for file system processing.  The receiver's RPC
          layer encounters no overhead from skipping over pad bytes, and the
          RDMA layer's high performance makes the insertion and transmission of
          padding on the sender a significant optimization.  In this way, the
          need for servers to perform RDMA Read to satisfy all but the largest
          client writes is obviated.  An added benefit is the reduction of
          message round trips on the network -- a potentially good trade, where
          latency is present.
        </t>
  
        <t>
          The value to choose for padding is subject to a number of criteria.
          A primary source of variable-length data in the RPC header is the
          authentication information, the form of which is client-determined,
          possibly in response to server specification.  The contents of
          COMPOUNDs, sizes of strings such as those passed to RENAME, etc. all
          go into the determination of a maximal NFSv4.1 request size and
          therefore minimal buffer size.  The client must select its offered
          value carefully, so as to avoid overburdening the server, and vice
          versa.  The benefit of an appropriate padding value is higher
          performance.
        </t>
  
        <figure>
          <artwork>
<![CDATA[
                    Sender gather:
        |RPC Request|Pad  bytes|Length| -> |User data...|
        \------+----------------------/      \
                \                             \
                 \    Receiver scatter:        \-----------+- ...
            /-----+----------------\            \           \
            |RPC Request|Pad|Length|   ->  |FS buffer|->|FS buffer|->...
]]>
          </artwork>
        </figure>

        <t>
          In the above case, the server may recycle unused buffers to the next
          posted receive if unused by the actual received request, or may pass
          the now-complete buffers by reference for normal write processing.
          For a server that can make use of it, this removes any need for data
          copies of incoming data, without resorting to complicated end-to-end
          buffer advertisement and management.  This includes most kernel-based
          and integrated server designs, among many others.  The client may
          perform similar optimizations, if desired.
        </t>
      </section>

      <section toc='exclude' anchor="ss:core_infrastructure:DRaNT" title="Dual RDMA and Non-RDMA Transports">
        <t>
          Some RDMA transports (e.g., RFC 5040 <xref target="RFC5040" />) permit a "streaming"
          (non-RDMA) phase, where ordinary traffic might flow before "stepping up"
          to RDMA mode, commencing RDMA traffic.  Some RDMA transports start
          connections always in RDMA mode.  NFSv4.1 allows, but does not
          assume, a streaming phase before RDMA mode.  When a connection is
          associated with a session, the client and server negotiate whether
          the connection is used in RDMA or non-RDMA mode (see Sections <xref target="op_CREATE_SESSION" format='counter' />
          and <xref target="op_BIND_CONN_TO_SESSION" format='counter' />).
        </t>
      </section>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:SS1" title="Session Security">
      <section toc='exclude' anchor="ss:core_infrastructure:SCS" title="Session Callback Security">
        <t>
          Via session/connection association, NFSv4.1 improves security over
          that provided by NFSv4.0 for the backchannel.  The connection is
          client-initiated (see <xref target="op_BIND_CONN_TO_SESSION" />) and subject to the same firewall
          and routing checks as the fore channel.  At the client's option (see
          <xref target="op_EXCHANGE_ID" />), connection association is fully authenticated before
          being activated (see <xref target="op_BIND_CONN_TO_SESSION" />).  Traffic from the server over
          the backchannel is authenticated exactly as the client specifies (see
          <xref target="ss:core_infrastructure:BRS" />).
        </t>
      </section>

      <section toc='exclude' anchor="ss:core_infrastructure:BRS" title="Backchannel RPC Security">
        <t>
          When the NFSv4.1 client establishes the backchannel, it informs the
          server of the security flavors and principals to use when sending
          requests.  If the security flavor is RPCSEC_GSS, the client expresses
          the principal in the form of an established RPCSEC_GSS context.  The
          server is free to use any of the flavor/principal combinations the
          client offers, but it MUST NOT use unoffered combinations.  This way,
          the client need not provide a target GSS principal for the
          backchannel as it did with NFSv4.0, nor does the server have to
          implement an RPCSEC_GSS initiator as it did with NFSv4.0 <xref target="RFC3530" />.
        </t>

        <t>
          The CREATE_SESSION (<xref target="op_CREATE_SESSION" />) and BACKCHANNEL_CTL
          (<xref target="op_BACKCHANNEL_CTL" />) operations allow the client to specify flavor/
          principal combinations.
        </t>

        <t>
          Also note that the SP4_SSV state protection mode (see Sections <xref target="op_EXCHANGE_ID" format='counter' />
          and <xref target="ss:core_infrastructure:PfUSC" format='counter' />) has the side benefit of providing SSV-derived
          RPCSEC_GSS contexts (<xref target="ss:core_infrastructure:TSSVGM" />).
        </t>
      </section>

      <section toc='exclude' anchor="ss:core_infrastructure:PfUSC" title="Protection from Unauthorized State Changes">
        <t>
          As described to this point in the specification, the state model of
          NFSv4.1 is vulnerable to an attacker that sends a SEQUENCE operation
          with a forged session ID and with a slot ID that it expects the
          legitimate client to use next.  When the legitimate client uses the
          slot ID with the same sequence number, the server returns the
          attacker's result from the reply cache, which disrupts the legitimate
          client and thus denies service to it.  Similarly, an attacker could
          send a CREATE_SESSION with a forged client ID to create a new session
          associated with the client ID.  The attacker could send requests
          using the new session that change locking state, such as LOCKU
          operations to release locks the legitimate client has acquired.
          Setting a security policy on the file that requires RPCSEC_GSS
          credentials when manipulating the file's state is one potential work
          around, but has the disadvantage of preventing a legitimate client
          from releasing state when RPCSEC_GSS is required to do so, but a GSS
          context cannot be obtained (possibly because the user has logged off
          the client).
        </t>

        <t>
          NFSv4.1 provides three options to a client for state protection,
          which are specified when a client creates a client ID via EXCHANGE_ID
          (<xref target="op_EXCHANGE_ID" />).
        </t>

        <t>
          The first (SP4_NONE) is to simply waive state protection.
        </t>

        <t>
          The other two options (SP4_MACH_CRED and SP4_SSV) share several
          traits:
        </t>

        <t>
          <list style='symbols'>
            <t>
              An RPCSEC_GSS-based credential is used to authenticate client ID
              and session maintenance operations, including creating and
              destroying a session, associating a connection with the session,
              and destroying the client ID.
            </t>

            <t>
              Because RPCSEC_GSS is used to authenticate client ID and session
              maintenance, the attacker cannot associate a rogue connection with
              a legitimate session, or associate a rogue session with a
              legitimate client ID in order to maliciously alter the client ID's
              lock state via CLOSE, LOCKU, DELEGRETURN, LAYOUTRETURN, etc.
            </t>

            <t>
              In cases where the server's security policies on a portion of its
              namespace require RPCSEC_GSS authentication, a client may have to
              use an RPCSEC_GSS credential to remove per-file state (e.g.,
              LOCKU, CLOSE, etc.).  The server may require that the principal
              that removes the state match certain criteria (e.g., the principal
              might have to be the same as the one that acquired the state).
              However, the client might not have an RPCSEC_GSS context for such
              a principal, and might not be able to create such a context
              (perhaps because the user has logged off).  When the client
              establishes SP4_MACH_CRED or SP4_SSV protection, it can specify a
              list of operations that the server MUST allow using the machine
              credential (if SP4_MACH_CRED is used) or the SSV credential (if
              SP4_SSV is used).
            </t>
          </list>
        </t>

        <t>
          The SP4_MACH_CRED state protection option uses a machine credential
          where the principal that creates the client ID MUST also be the
          principal that performs client ID and session maintenance operations.
          The security of the machine credential state protection approach
          depends entirely on safe guarding the per-machine credential.
          Assuming a proper safeguard using the per-machine credential for
          operations like CREATE_SESSION, BIND_CONN_TO_SESSION,
          DESTROY_SESSION, and DESTROY_CLIENTID will prevent an attacker from
          associating a rogue connection with a session, or associating a rogue
          session with a client ID.
        </t>

        <t>
          There are at least three scenarios for the SP4_MACH_CRED option:
        </t>

        <t>
          <list style='numbers'>
            <t>
              The system administrator configures a unique, permanent per-machine
              credential for one of the mandated GSS mechanisms (e.g.,
              if Kerberos V5 is used, a "keytab" containing a principal derived
              from a client host name could be used).
            </t>

            <t>
              The client is used by a single user, and so the client ID and its
              sessions are used by just that user.  If the user's credential
              expires, then session and client ID maintenance cannot occur, but
              since the client has a single user, only that user is
              inconvenienced.
            </t>

            <t>
              The physical client has multiple users, but the client
              implementation has a unique client ID for each user.  This is
              effectively the same as the second scenario, but a disadvantage
              is that each user needs to be allocated at least one session
              each, so the approach suffers from lack of economy.
            </t>
          </list>
        </t>

        <t>
          The SP4_SSV protection option uses the SSV (<xref target="ss:introduction:GD" />), via
          RPCSEC_GSS and the SSV GSS mechanism (<xref target="ss:core_infrastructure:TSSVGM" />), to protect
          state from attack.  The SP4_SSV protection option is intended for the
          situation comprised of a client that has multiple active users and a
          system administrator who wants to avoid the burden of installing a
          permanent machine credential on each client.  The SSV is established
          and updated on the server via SET_SSV (see <xref target="op_SET_SSV" />).  To
          prevent eavesdropping, a client SHOULD send SET_SSV via RPCSEC_GSS
          with the privacy service.  Several aspects of the SSV make it
          intractable for an attacker to guess the SSV, and thus associate
          rogue connections with a session, and rogue sessions with a client
          ID:
        </t>

        <t>
          <list style='symbols'>
            <t>
              The arguments to and results of SET_SSV include digests of the old
              and new SSV, respectively.
            </t>

            <t>
              Because the initial value of the SSV is zero, therefore known, the
              client that opts for SP4_SSV protection and opts to apply SP4_SSV
              protection to BIND_CONN_TO_SESSION and CREATE_SESSION MUST send at
              least one SET_SSV operation before the first BIND_CONN_TO_SESSION
              operation or before the second CREATE_SESSION operation on a
              client ID.  If it does not, the SSV mechanism will not generate
              tokens (<xref target="ss:core_infrastructure:TSSVGM" />).  A client SHOULD send SET_SSV as soon as
              a session is created.
            </t>

            <t>
              A SET_SSV request does not replace the SSV with the argument to
              SET_SSV.  Instead, the current SSV on the server is logically
              exclusive ORed (XORed) with the argument to SET_SSV.  Each time a
              new principal uses a client ID for the first time, the client
              SHOULD send a SET_SSV with that principal's RPCSEC_GSS
              credentials, with RPCSEC_GSS service set to RPC_GSS_SVC_PRIVACY.
            </t>
          </list>
        </t>

        <t>
          Here are the types of attacks that can be attempted by an attacker
          named Eve on a victim named Bob, and how SP4_SSV protection foils
          each attack:
        </t>

        <t>
          <list style='symbols'>
            <t>
              Suppose Eve is the first user to log into a legitimate client.
              Eve's use of an NFSv4.1 file system will cause the legitimate
              client to create a client ID with SP4_SSV protection, specifying
              that the BIND_CONN_TO_SESSION operation MUST use the SSV
              credential.  Eve's use of the file system also causes an SSV to be
              created.  The SET_SSV operation that creates the SSV will be
              protected by the RPCSEC_GSS context created by the legitimate
              client, which uses Eve's GSS principal and credentials.  Eve can
              eavesdrop on the network while her RPCSEC_GSS context is created
              and the SET_SSV using her context is sent.  Even if the legitimate
              client sends the SET_SSV with RPC_GSS_SVC_PRIVACY, because Eve
              knows her own credentials, she can decrypt the SSV.  Eve can
              compute an RPCSEC_GSS credential that BIND_CONN_TO_SESSION will
              accept, and so associate a new connection with the legitimate
              session.  Eve can change the slot ID and sequence state of a
              legitimate session, and/or the SSV state, in such a way that when
              Bob accesses the server via the same legitimate client, the
              legitimate client will be unable to use the session.
              <vspace blankLines="1"/>
              The client's only recourse is to create a new client ID for Bob to
              use, and establish a new SSV for the client ID.  The client will
              be unable to delete the old client ID, and will let the lease on
              the old client ID expire.
              <vspace blankLines="1"/>
              Once the legitimate client establishes an SSV over the new session
              using Bob's RPCSEC_GSS context, Eve can use the new session via
              the legitimate client, but she cannot disrupt Bob.  Moreover,
              because the client SHOULD have modified the SSV due to Eve using
              the new session, Bob cannot get revenge on Eve by associating a
              rogue connection with the session.
              <vspace blankLines="1"/>
              The question is how did the legitimate client detect that Eve has
              hijacked the old session?  When the client detects that a new
              principal, Bob, wants to use the session, it SHOULD have sent a
              SET_SSV, which leads to the following sub-scenarios:
              <vspace blankLines="1"/>
              <list style='symbols'>
                <t>
                  Let us suppose that from the rogue connection, Eve sent a
                  SET_SSV with the same slot ID and sequence ID that the
                  legitimate client later uses.  The server will assume the
                  SET_SSV sent with Bob's credentials is a retry, and return to
                  the legitimate client the reply it sent Eve.  However, unless
                  Eve can correctly guess the SSV the legitimate client will use,
                  the digest verification checks in the SET_SSV response will
                  fail.  That is an indication to the client that the session has
                  apparently been hijacked.
                </t>

                <t>
                  Alternatively, Eve sent a SET_SSV with a different slot ID than
                  the legitimate client uses for its SET_SSV.  Then the digest
                  verification of the SET_SSV sent with Bob's credentials fails
                  on the server, and the error returned to the client makes it
                  apparent that the session has been hijacked.
                </t>

                <t>
                  Alternatively, Eve sent an operation other than SET_SSV, but
                  with the same slot ID and sequence that the legitimate client
                  uses for its SET_SSV.  The server returns to the legitimate
                  client the response it sent Eve.  The client sees that the
                  response is not at all what it expects.  The client assumes
                  either session hijacking or a server bug, and either way
                  destroys the old session.
                </t>
              </list>
            </t>

            <t>
              Eve associates a rogue connection with the session as above, and
              then destroys the session.  Again, Bob goes to use the server from
              the legitimate client, which sends a SET_SSV using Bob's
              credentials.  The client receives an error that indicates that the
              session does not exist.  When the client tries to create a new
              session, this will fail because the SSV it has does not match that
              which the server has, and now the client knows the session was
              hijacked.  The legitimate client establishes a new client ID.
            </t>

            <t>
              If Eve creates a connection before the legitimate client
              establishes an SSV, because the initial value of the SSV is zero
              and therefore known, Eve can send a SET_SSV that will pass the
              digest verification check.  However, because the new connection
              has not been associated with the session, the SET_SSV is rejected
              for that reason.
            </t>
          </list>
        </t>

        <t>
          In summary, an attacker's disruption of state when SP4_SSV protection
          is in use is limited to the formative period of a client ID, its
          first session, and the establishment of the SSV.  Once a non-malicious
          user uses the client ID, the client quickly detects any
          hijack and rectifies the situation.  Once a non-malicious user
          successfully modifies the SSV, the attacker cannot use NFSv4.1
          operations to disrupt the non-malicious user.
        </t>

        <t>
          Note that neither the SP4_MACH_CRED nor SP4_SSV protection approaches
          prevent hijacking of a transport connection that has previously been
          associated with a session.  If the goal of a counter-threat strategy
          is to prevent connection hijacking, the use of IPsec is RECOMMENDED.
        </t>

        <t>
          If a connection hijack occurs, the hijacker could in theory change
          locking state and negatively impact the service to legitimate
          clients.  However, if the server is configured to require the use of
          RPCSEC_GSS with integrity or privacy on the affected file objects,
          and if EXCHGID4_FLAG_BIND_PRINC_STATEID capability (<xref target="op_EXCHANGE_ID" />) is
          in force, this will thwart unauthorized attempts to change locking
          state.
        </t>
      </section>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:TSSVGM" title="The Secret State Verifier (SSV) GSS Mechanism">
      <t>
        The SSV provides the secret key for a GSS mechanism internal to
        NFSv4.1 that NFSv4.1 uses for state protection.  Contexts for this
        mechanism are not established via the RPCSEC_GSS protocol.  Instead,
        the contexts are automatically created when EXCHANGE_ID specifies
        SP4_SSV protection.  The only tokens defined are the PerMsgToken
        (emitted by GSS_GetMIC) and the SealedMessage token (emitted by
        GSS_Wrap).
      </t>
     
      <t>
        The mechanism OID for the SSV mechanism is
        iso.org.dod.internet.private.enterprise.Michael Eisler.nfs.ssv_mech
        (1.3.6.1.4.1.28882.1.1).  While the SSV mechanism does not define any
        initial context tokens, the OID can be used to let servers indicate
        that the SSV mechanism is acceptable whenever the client sends a
        SECINFO or SECINFO_NO_NAME operation (see <xref target="ss:core_infrastructure:SSN" />).
      </t>
     
      <t>
        The SSV mechanism defines four subkeys derived from the SSV value.
        Each time SET_SSV is invoked, the subkeys are recalculated by the
        client and server.  The calculation of each of the four subkeys
        depends on each of the four respective ssv_subkey4 enumerated values.
        The calculation uses the HMAC <xref target="RFC2104" /> algorithm, using the current SSV
        as the key, the one-way hash algorithm as negotiated by EXCHANGE_ID,
        and the input text as represented by the XDR encoded enumeration
        value for that subkey of data type ssv_subkey4.  If the length of the
        output of the HMAC algorithm exceeds the length of key of the
        encryption algorithm (which is also negotiated by EXCHANGE_ID), then
        the subkey MUST be truncated from the HMAC output, i.e., if the
        subkey is of N bytes long, then the first N bytes of the HMAC output
        MUST be used for the subkey.  The specification of EXCHANGE_ID states
        that the length of the output of the HMAC algorithm MUST NOT be less
        than the length of subkey needed for the encryption algorithm (see
        <xref target="op_EXCHANGE_ID" />).
      </t>
     
      <?rfc include="autogen/type_ssv_subkey4.xml"?>

      <t>
        The subkey derived from SSV4_SUBKEY_MIC_I2T is used for calculating
        message integrity codes (MICs) that originate from the NFSv4.1
        client, whether as part of a request over the fore channel or a
        response over the backchannel.  The subkey derived from
        SSV4_SUBKEY_MIC_T2I is used for MICs originating from the NFSv4.1
        server.  The subkey derived from SSV4_SUBKEY_SEAL_I2T is used for
        encryption text originating from the NFSv4.1 client, and the subkey
        derived from SSV4_SUBKEY_SEAL_T2I is used for encryption text
        originating from the NFSv4.1 server.
      </t>
     
      <t>
        The PerMsgToken description is based on an XDR definition:
      </t>
     
      <?rfc include="autogen/type_ssv_mic_plain_tkn4.xml"?>
      <?rfc include="autogen/type_ssv_mic_tkn4.xml"?>

      <t>
        The field smt_hmac is an HMAC calculated by using the subkey derived
        from SSV4_SUBKEY_MIC_I2T or SSV4_SUBKEY_MIC_T2I as the key, the
        one-way hash algorithm as negotiated by EXCHANGE_ID, and the input text
        as represented by data of type ssv_mic_plain_tkn4.  The field
        smpt_ssv_seq is the same as smt_ssv_seq.  The field smpt_orig_plain
        is the "message" input passed to GSS_GetMIC() (see Section 2.3.1 of
        <xref target="RFC2743" />).  The caller of GSS_GetMIC() provides a pointer to a buffer
        containing the plain text.  The SSV mechanism's entry point for
        GSS_GetMIC() encodes this into an opaque array, and the encoding will
        include an initial four-byte length, plus any necessary padding.
        Prepended to this will be the XDR encoded value of smpt_ssv_seq, thus
        making up an XDR encoding of a value of data type ssv_mic_plain_tkn4,
        which in turn is the input into the HMAC.
      </t>
     
      <t>
        The token emitted by GSS_GetMIC() is XDR encoded and of XDR data type
        ssv_mic_tkn4.  The field smt_ssv_seq comes from the SSV sequence
        number, which is equal to one after SET_SSV (<xref target="op_SET_SSV" />) is called
        the first time on a client ID.  Thereafter, the SSV sequence number
        is incremented on each SET_SSV.  Thus, smt_ssv_seq represents the
        version of the SSV at the time GSS_GetMIC() was called.  As noted in
        <xref target="op_EXCHANGE_ID" />, the client and server can maintain multiple concurrent
        versions of the SSV.  This allows the SSV to be changed without
        serializing all RPC calls that use the SSV mechanism with SET_SSV
        operations.  Once the HMAC is calculated, it is XDR encoded into
        smt_hmac, which will include an initial four-byte length, and any
        necessary padding.  Prepended to this will be the XDR encoded value
        of smt_ssv_seq.
      </t>
     
      <t>
        The SealedMessage description is based on an XDR definition:
      </t>
     
      <?rfc include="autogen/type_ssv_seal_plain_tkn4.xml"?>
      <?rfc include="autogen/type_ssv_seal_cipher_tkn4.xml"?>

      <t>
        The token emitted by GSS_Wrap() is XDR encoded and of XDR data type
        ssv_seal_cipher_tkn4.
      </t>
     
      <t>
        The ssct_ssv_seq field has the same meaning as smt_ssv_seq.
      </t>
     
      <t>
        The ssct_encr_data field is the result of encrypting a value of the
        XDR encoded data type ssv_seal_plain_tkn4.  The encryption key is the
        subkey derived from SSV4_SUBKEY_SEAL_I2T or SSV4_SUBKEY_SEAL_T2I, and
        the encryption algorithm is that negotiated by EXCHANGE_ID.
      </t>
     
      <t>
        The ssct_iv field is the initialization vector (IV) for the
        encryption algorithm (if applicable) and is sent in clear text.  The
        content and size of the IV MUST comply with the specification of the
        encryption algorithm.  For example, the id-aes256-CBC algorithm MUST
        use a 16-byte initialization vector (IV), which MUST be unpredictable
        for each instance of a value of data type ssv_seal_plain_tkn4 that is
        encrypted with a particular SSV key.
      </t>
     
      <t>
        The ssct_hmac field is the result of computing an HMAC using the
        value of the XDR encoded data type ssv_seal_plain_tkn4 as the input
        text.  The key is the subkey derived from SSV4_SUBKEY_MIC_I2T or
        SSV4_SUBKEY_MIC_T2I, and the one-way hash algorithm is that
        negotiated by EXCHANGE_ID.
      </t>
     
      <t>
        The sspt_confounder field is a random value.
      </t>
     
      <t>
        The sspt_ssv_seq field is the same as ssvt_ssv_seq.
      </t>
     
      <t>
        The field sspt_orig_plain field is the original plaintext and is the
        "input_message" input passed to GSS_Wrap() (see Section 2.3.3 of
        <xref target="RFC2743" />).  As with the handling of the plaintext by the SSV mechanism's
        GSS_GetMIC() entry point, the entry point for GSS_Wrap() expects a
        pointer to the plaintext, and will XDR encode an opaque array into
        sspt_orig_plain representing the plain text, along with the other
        fields of an instance of data type ssv_seal_plain_tkn4.
      </t>
     
      <t>
        The sspt_pad field is present to support encryption algorithms that
        require inputs to be in fixed-sized blocks.  The content of sspt_pad
        is zero filled except for the length.  Beware that the XDR encoding
        of ssv_seal_plain_tkn4 contains three variable-length arrays, and so
        each array consumes four bytes for an array length, and each array
        that follows the length is always padded to a multiple of four bytes
        per the XDR standard.
      </t>
     
      <t>
        For example, suppose the encryption algorithm uses 16-byte blocks,
        and the sspt_confounder is three bytes long, and the sspt_orig_plain
        field is 15 bytes long.  The XDR encoding of sspt_confounder uses
        eight bytes (4 + 3 + 1 byte pad), the XDR encoding of sspt_ssv_seq
        uses four bytes, the XDR encoding of sspt_orig_plain uses 20 bytes (4
        + 15 + 1 byte pad), and the smallest XDR encoding of the sspt_pad
        field is four bytes.  This totals 36 bytes.  The next multiple of 16
        is 48; thus, the length field of sspt_pad needs to be set to 12
        bytes, or a total encoding of 16 bytes.  The total number of XDR
        encoded bytes is thus 8 + 4 + 20 + 16 = 48.
      </t>
     
      <t>
        GSS_Wrap() emits a token that is an XDR encoding of a value of data
        type ssv_seal_cipher_tkn4.  Note that regardless of whether or not
        the caller of GSS_Wrap() requests confidentiality, the token always
        has confidentiality.  This is because the SSV mechanism is for
        RPCSEC_GSS, and RPCSEC_GSS never produces GSS_wrap() tokens without
        confidentiality.
      </t>
     
      <t>
        There is one SSV per client ID.  There is a single GSS context for a
        client ID / SSV pair.  All SSV mechanism RPCSEC_GSS handles of a
        client ID / SSV pair share the same GSS context.  SSV GSS contexts do
        not expire except when the SSV is destroyed (causes would include the
        client ID being destroyed or a server restart).  Since one purpose of
        context expiration is to replace keys that have been in use for "too
        long", hence vulnerable to compromise by brute force or accident, the
        client can replace the SSV key by sending periodic SET_SSV
        operations, which is done by cycling through different users'
        RPCSEC_GSS credentials.  This way, the SSV is replaced without
        destroying the SSV's GSS contexts.
      </t>
     
      <t>
        SSV RPCSEC_GSS handles can be expired or deleted by the server at any
        time, and the EXCHANGE_ID operation can be used to create more SSV
        RPCSEC_GSS handles.  Expiration of SSV RPCSEC_GSS handles does not
        imply that the SSV or its GSS context has expired.
      </t>
     
      <t>
        The client MUST establish an SSV via SET_SSV before the SSV GSS
        context can be used to emit tokens from GSS_Wrap() and GSS_GetMIC().
        If SET_SSV has not been successfully called, attempts to emit tokens
        MUST fail.
      </t>
     
      <t>
        The SSV mechanism does not support replay detection and sequencing in
        its tokens because RPCSEC_GSS does not use those features (See
        <xref target="ss:file_attributes:RA1" />.2, "Context Creation Requests", in <xref target="RFC2203" />).  However,
        <xref target="ss:core_infrastructure:SCfRWUtSM" /> discusses special considerations for the SSV
        mechanism when used with RPCSEC_GSS.
      </t>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:SCfRWUtSM" title="Security Considerations for RPCSEC_GSS When Using the SSV Mechanism">
      <t>
        When a client ID is created with SP4_SSV state protection (see
        <xref target="op_EXCHANGE_ID" />), the client is permitted to associate multiple
        RPCSEC_GSS handles with the single SSV GSS context (see
        <xref target="ss:core_infrastructure:TSSVGM" />).  Because of the way RPCSEC_GSS (both version 1 and
        version 2, see <xref target="RFC2203" /> and <xref target="RFC5403" />) calculate the verifier of the reply,
        special care must be taken by the implementation of the NFSv4.1
        client to prevent attacks by a man-in-the-middle.  The verifier of an
        RPCSEC_GSS reply is the output of GSS_GetMIC() applied to the input
        value of the seq_num field of the RPCSEC_GSS credential (data type
        rpc_gss_cred_ver_1_t) (see Section 5.3.3.2 of <xref target="RFC2203" />).  If multiple
        RPCSEC_GSS handles share the same GSS context, then if one handle is
        used to send a request with the same seq_num value as another handle,
        an attacker could block the reply, and replace it with the verifier
        used for the other handle.
      </t>

      <t>
        There are multiple ways to prevent the attack on the SSV RPCSEC_GSS
        verifier in the reply.  The simplest is believed to be as follows.
      </t>

      <t>
        <list style='symbols'>
          <t>
            Each time one or more new SSV RPCSEC_GSS handles are created via
            EXCHANGE_ID, the client SHOULD send a SET_SSV operation to modify
            the SSV.  By changing the SSV, the new handles will not result in
            the re-use of an SSV RPCSEC_GSS verifier in a reply.
          </t>

          <t>
            When a requester decides to use N SSV RPCSEC_GSS handles, it
            SHOULD assign a unique and non-overlapping range of seq_nums to
            each SSV RPCSEC_GSS handle.  The size of each range SHOULD be
            equal to MAXSEQ / N (see Section 5 of <xref target="RFC2203" /> for the definition of
            MAXSEQ).  When an SSV RPCSEC_GSS handle reaches its maximum, it
            SHOULD force the replier to destroy the handle by sending a NULL
            RPC request with seq_num set to MAXSEQ + 1 (see Section 5.3.3.3 of
            <xref target="RFC2203" />).
          </t>

          <t>
            When the requester wants to increase or decrease N, it SHOULD
            force the replier to destroy all N handles by sending a NULL RPC
            request on each handle with seq_num set to MAXSEQ + 1.  If the
            requester is the client, it SHOULD send a SET_SSV operation before
            using new handles.  If the requester is the server, then the
            client SHOULD send a SET_SSV operation when it detects that the
            server has forced it to destroy a backchannel's SSV RPCSEC_GSS
            handle.  By sending a SET_SSV operation, the SSV will change, and
            so the attacker will be unavailable to successfully replay a
            previous verifier in a reply to the requester.
          </t>
        </list>
      </t>

      <t>
        Note that if the replier carefully creates the SSV RPCSEC_GSS
        handles, the related risk of a man-in-the-middle splicing a forged
        SSV RPCSEC_GSS credential with a verifier for another handle does not
        exist.  This is because the verifier in an RPCSEC_GSS request is
        computed from input that includes both the RPCSEC_GSS handle and
        seq_num (see Section 5.3.1 of <xref target="RFC2203" />).  Provided the replier takes care
        to avoid re-using the value of an RPCSEC_GSS handle that it creates,
        such as by including a generation number in the handle, the
        man-in-the-middle will not be able to successfully replay a previous
        verifier in the request to a replier.
      </t>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:SMSS" title="Session Mechanics - Steady State">
      <section toc='exclude' anchor="ss:core_infrastructure:OotS" title="Obligations of the Server">
        <t>
          The server has the primary obligation to monitor the state of
          backchannel resources that the client has created for the server
          (RPCSEC_GSS contexts and backchannel connections).  If these
          resources vanish, the server takes action as specified in
          <xref target="ss:core_infrastructure:ERSA" />.
        </t>
      </section>

      <section toc='exclude' anchor="ss:core_infrastructure:OotC" title="Obligations of the Client">
        <t>
          The client SHOULD honor the following obligations in order to utilize
          the session:
        </t>

        <t>
          <list style='symbols'>
            <t>
              Keep a necessary session from going idle on the server.  A client
              that requires a session but nonetheless is not sending operations
              risks having the session be destroyed by the server.  This is
              because sessions consume resources, and resource limitations may
              force the server to cull an inactive session.  A server MAY
              consider a session to be inactive if the client has not used the
              session before the session inactivity timer (<xref target="ss:core_infrastructure:SIT" />) has
              expired.
            </t>

            <t>
              Destroy the session when not needed.  If a client has multiple
              sessions, one of which has no requests waiting for replies, and
              has been idle for some period of time, it SHOULD destroy the
              session.
            </t>

            <t>
              Maintain GSS contexts and RPCSEC_GSS handles for the backchannel.
              If the client requires the server to use the RPCSEC_GSS security
              flavor for callbacks, then it needs to be sure the RPCSEC_GSS
              handles and/or their GSS contexts that are handed to the server
              via BACKCHANNEL_CTL or CREATE_SESSION are unexpired.
            </t>

            <t>
              Preserve a connection for a backchannel.  The server requires a
              backchannel in order to gracefully recall recallable state or
              notify the client of certain events.  Note that if the connection
              is not being used for the fore channel, there is no way for the
              client to tell if the connection is still alive (e.g., the server
              restarted without sending a disconnect).  The onus is on the
              server, not the client, to determine if the backchannel's
              connection is alive, and to indicate in the response to a SEQUENCE
              operation when the last connection associated with a session's
              backchannel has disconnected.
            </t>
          </list>
        </t>
      </section>

      <section toc='exclude' anchor="ss:core_infrastructure:StCTtES" title="Steps the Client Takes to Establish a Session">
        <t>
          If the client does not have a client ID, the client sends EXCHANGE_ID
          to establish a client ID.  If it opts for SP4_MACH_CRED or SP4_SSV
          protection, in the spo_must_enforce list of operations, it SHOULD at
          minimum specify CREATE_SESSION, DESTROY_SESSION,
          BIND_CONN_TO_SESSION, BACKCHANNEL_CTL, and DESTROY_CLIENTID.  If it
          opts for SP4_SSV protection, the client needs to ask for SSV-based
          RPCSEC_GSS handles.
        </t>

        <t>
          The client uses the client ID to send a CREATE_SESSION on a
          connection to the server.  The results of CREATE_SESSION indicate
          whether or not the server will persist the session reply cache
          through a server that has restarted, and the client notes this for
          future reference.
        </t>

        <t>
          If the client specified SP4_SSV state protection when the client ID
          was created, then it SHOULD send SET_SSV in the first COMPOUND after
          the session is created.  Each time a new principal goes to use the
          client ID, it SHOULD send a SET_SSV again.
        </t>

        <t>
          If the client wants to use delegations, layouts, directory
          notifications, or any other state that requires a backchannel, then
          it needs to add a connection to the backchannel if CREATE_SESSION did
          not already do so.  The client creates a connection, and calls
          BIND_CONN_TO_SESSION to associate the connection with the session and
          the session's backchannel.  If CREATE_SESSION did not already do so,
          the client MUST tell the server what security is required in order
          for the client to accept callbacks.  The client does this via
          BACKCHANNEL_CTL.  If the client selected SP4_MACH_CRED or SP4_SSV
          protection when it called EXCHANGE_ID, then the client SHOULD specify
          that the backchannel use RPCSEC_GSS contexts for security.
        </t>

        <t>
          If the client wants to use additional connections for the
          backchannel, then it needs to call BIND_CONN_TO_SESSION on each
          connection it wants to use with the session.  If the client wants to
          use additional connections for the fore channel, then it needs to
          call BIND_CONN_TO_SESSION if it specified SP4_SSV or SP4_MACH_CRED
          state protection when the client ID was created.
        </t>

        <t>
          At this point, the session has reached steady state.
        </t>
      </section>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:SIT" title="Session Inactivity Timer">
      <t>
        The server MAY maintain a session inactivity timer for each session.
        If the session inactivity timer expires, then the server MAY destroy
        the session.  To avoid losing a session due to inactivity, the client
        MUST renew the session inactivity timer.  The length of session
        inactivity timer MUST NOT be less than the lease_time attribute
        (<xref target="ss:file_attributes:A1l" />).  As with lease renewal (<xref target="ss:state_management:LR" />), when the
        server receives a SEQUENCE operation, it resets the session
        inactivity timer, and MUST NOT allow the timer to expire while the
        rest of the operations in the COMPOUND procedure's request are still
        executing.  Once the last operation has finished, the server MUST set
        the session inactivity timer to expire no sooner than the sum of the
        current time and the value of the lease_time attribute.
      </t>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:SMR" title="Session Mechanics - Recovery">
      <section toc='exclude' anchor="ss:core_infrastructure:ERCA" title="Events Requiring Client Action">
        <t>
          The following events require client action to recover.
        </t>

        <section toc='exclude' anchor="ss:core_infrastructure:RCLbCP" title="RPCSEC_GSS Context Loss by Callback Path">
          <t>
            If all RPCSEC_GSS handles granted by the client to the server for
            callback use have expired, the client MUST establish a new handle via
            BACKCHANNEL_CTL.  The sr_status_flags field of the SEQUENCE results
            indicates when callback handles are nearly expired, or fully expired
            (see <xref target="ss:op_SEQUENCE:D" />).
          </t>
        </section>

        <section toc='exclude' anchor="ss:core_infrastructure:CL" title="Connection Loss">
          <t>
            If the client loses the last connection of the session and wants to
            retain the session, then it needs to create a new connection, and if,
            when the client ID was created, BIND_CONN_TO_SESSION was specified in
            the spo_must_enforce list, the client MUST use BIND_CONN_TO_SESSION
            to associate the connection with the session.
          </t>

          <t>
            If there was a request outstanding at the time of connection loss,
            then if the client wants to continue to use the session, it MUST
            retry the request, as described in <xref target="ss:core_infrastructure:RaRoR" />.  Note that it is
            not necessary to retry requests over a connection with the same
            source network address or the same destination network address as the
            lost connection.  As long as the session ID, slot ID, and sequence ID
            in the retry match that of the original request, the server will
            recognize the request as a retry if it executed the request prior to
            disconnect.
          </t>

          <t>
            If the connection that was lost was the last one associated with the
            backchannel, and the client wants to retain the backchannel and/or
            prevent revocation of recallable state, the client needs to
            reconnect, and if it does, it MUST associate the connection to the
            session and backchannel via BIND_CONN_TO_SESSION.  The server SHOULD
            indicate when it has no callback connection via the sr_status_flags
            result from SEQUENCE.
          </t>
        </section>

        <section toc='exclude' anchor="ss:core_infrastructure:BGCL" title="Backchannel GSS Context Loss">
          <t>
            Via the sr_status_flags result of the SEQUENCE operation or other
            means, the client will learn if some or all of the RPCSEC_GSS
            contexts it assigned to the backchannel have been lost.  If the
            client wants to retain the backchannel and/or not put recallable
            state subject to revocation, the client needs to use BACKCHANNEL_CTL
            to assign new contexts.
          </t>
        </section>

        <section toc='exclude' anchor="ss:core_infrastructure:LoS" title="Loss of Session">
          <t>
            The replier might lose a record of the session.  Causes include:
          </t>

          <t>
            <list style='symbols'>
              <t>
                Replier failure and restart.
              </t>

              <t>
                A catastrophe that causes the reply cache to be corrupted or lost
                on the media on which it was stored.  This applies even if the
                replier indicated in the CREATE_SESSION results that it would
                persist the cache.
              </t>

              <t>
                The server purges the session of a client that has been inactive
                for a very extended period of time.
              </t>

              <t>
                As a result of configuration changes among a set of clustered
                servers, a network address previously connected to one server
                becomes connected to a different server that has no knowledge of
                the session in question.  Such a configuration change will
                generally only happen when the original server ceases to function
                for a time.
              </t>
            </list>
          </t>

          <t>
            Loss of reply cache is equivalent to loss of session.  The replier
            indicates loss of session to the requester by returning
            NFS4ERR_BADSESSION on the next operation that uses the session ID
            that refers to the lost session.
          </t>

          <t>
            After an event like a server restart, the client may have lost its
            connections.  The client assumes for the moment that the session has
            not been lost.  It reconnects, and if it specified connection
            association enforcement when the session was created, it invokes
            BIND_CONN_TO_SESSION using the session ID.  Otherwise, it invokes
            SEQUENCE.  If BIND_CONN_TO_SESSION or SEQUENCE returns
            NFS4ERR_BADSESSION, the client knows the session is not available to
            it when communicating with that network address.  If the connection
            survives session loss, then the next SEQUENCE operation the client
            sends over the connection will get back NFS4ERR_BADSESSION.  The
            client again knows the session was lost.
          </t>

          <t>
            Here is one suggested algorithm for the client when it gets
            NFS4ERR_BADSESSION.  It is not obligatory in that, if a client does
            not want to take advantage of such features as trunking, it may omit
            parts of it.  However, it is a useful example that draws attention to
            various possible recovery issues:
          </t>

          <t>
            <list style='numbers'>
              <t>
                If the client has other connections to other server network
                addresses associated with the same session, attempt a COMPOUND
                with a single operation, SEQUENCE, on each of the other
                connections.
              </t>

              <t>
                If the attempts succeed, the session is still alive, and this is
                a strong indicator that the server's network address has moved.
                The client might send an EXCHANGE_ID on the connection that
                returned NFS4ERR_BADSESSION to see if there are opportunities for
                client ID trunking (i.e., the same client ID and so_major are
                returned).  The client might use DNS to see if the moved network
                address was replaced with another, so that the performance and
                availability benefits of session trunking can continue.
              </t>

              <t>
                If the SEQUENCE requests fail with NFS4ERR_BADSESSION, then the
                session no longer exists on any of the server network addresses
                for which the client has connections associated with that session
                ID.  It is possible the session is still alive and available on
                other network addresses.  The client sends an EXCHANGE_ID on all
                the connections to see if the server owner is still listening on
                those network addresses.  If the same server owner is returned
                but a new client ID is returned, this is a strong indicator of a
                server restart.  If both the same server owner and same client ID
                are returned, then this is a strong indication that the server
                did delete the session, and the client will need to send a
                CREATE_SESSION if it has no other sessions for that client ID.
                If a different server owner is returned, the client can use DNS
                to find other network addresses.  If it does not, or if DNS does
                not find any other addresses for the server, then the client will
                be unable to provide NFSv4.1 service, and fatal errors should be
                returned to processes that were using the server.  If the client
                is using a "mount" paradigm, unmounting the server is advised.
              </t>

              <t>
                If the client knows of no other connections associated with the
                session ID and server network addresses that are, or have been,
                associated with the session ID, then the client can use DNS to
                find other network addresses.  If it does not, or if DNS does not
                find any other addresses for the server, then the client will be
                unable to provide NFSv4.1 service, and fatal errors should be
                returned to processes that were using the server.  If the client
                is using a "mount" paradigm, unmounting the server is advised.
              </t>
            </list>
          </t>

          <t>
            If there is a reconfiguration event that results in the same network
            address being assigned to servers where the eir_server_scope value is
            different, it cannot be guaranteed that a session ID generated by the
            first will be recognized as invalid by the first.  Therefore, in
            managing server reconfigurations among servers with different server
            scope values, it is necessary to make sure that all clients have
            disconnected from the first server before effecting the
            reconfiguration.  Nonetheless, clients should not assume that servers
            will always adhere to this requirement; clients MUST be prepared to
            deal with unexpected effects of server reconfigurations.  Even where
            a session ID is inappropriately recognized as valid, it is likely
            either that the connection will not be recognized as valid or that a
            sequence value for a slot will not be correct.  Therefore, when a
            client receives results indicating such unexpected errors, the use of
            EXCHANGE_ID to determine the current server configuration is
            RECOMMENDED.
          </t>

          <t>
            A variation on the above is that after a server's network address
            moves, there is no NFSv4.1 server listening, e.g., no listener on
            port 2049.  In this example, one of the following occur: the NFSv4
            server returns NFS4ERR_MINOR_VERS_MISMATCH, the NFS server returns a
            PROG_MISMATCH error, the RPC listener on 2049 returns PROG_UNVAIL, or
            attempts to reconnect to the network address timeout.  These SHOULD
            be treated as equivalent to SEQUENCE returning NFS4ERR_BADSESSION for
            these purposes.
          </t>

          <t>
            When the client detects session loss, it needs to call CREATE_SESSION
            to recover.  Any non-idempotent operations that were in progress
            might have been performed on the server at the time of session loss.
            The client has no general way to recover from this.
          </t>

          <t>
            Note that loss of session does not imply loss of byte-range lock,
            open, delegation, or layout state because locks, opens, delegations,
            and layouts are tied to the client ID and depend on the client ID,
            not the session.  Nor does loss of byte-range lock, open, delegation,
            or layout state imply loss of session state, because the session
            depends on the client ID; loss of client ID however does imply loss
            of session, byte-range lock, open, delegation, and layout state.  See
            <xref target="ss:state_management:SFaR" />.  A session can survive a server restart, but lock
            recovery may still be needed.
          </t>

          <t>
            It is possible that CREATE_SESSION will fail with
            NFS4ERR_STALE_CLIENTID (e.g., the server restarts and does not
            preserve client ID state).  If so, the client needs to call
            EXCHANGE_ID, followed by CREATE_SESSION.
          </t>
        </section>
      </section>

      <section toc='exclude' anchor="ss:core_infrastructure:ERSA" title="Events Requiring Server Action">
        <t>
          The following events require server action to recover.
        </t>

        <section toc='exclude' anchor="ss:core_infrastructure:CCaR" title="Client Crash and Restart">
          <t>
            As described in <xref target="op_EXCHANGE_ID" />, a restarted client sends EXCHANGE_ID
            in such a way that it causes the server to delete any sessions it
            had.
          </t>
        </section>

        <section toc='exclude' anchor="ss:core_infrastructure:CCwNR" title="Client Crash with No Restart">
          <t>
            If a client crashes and never comes back, it will never send
            EXCHANGE_ID with its old client owner.  Thus, the server has session
            state that will never be used again.  After an extended period of
            time, and if the server has resource constraints, it MAY destroy the
            old session as well as locking state.
          </t>
        </section>

        <section toc='exclude' anchor="ss:core_infrastructure:ENP" title="Extended Network Partition">
          <t>
            To the server, the extended network partition may be no different
            from a client crash with no restart (see <xref target="ss:core_infrastructure:CCwNR" />).
            Unless the server can discern that there is a network partition, it
            is free to treat the situation as if the client has crashed
            permanently.
          </t>
        </section>

        <section toc='exclude' anchor="ss:core_infrastructure:BCL" title="Backchannel Connection Loss">
          <t>
            If there were callback requests outstanding at the time of a
            connection loss, then the server MUST retry the requests, as
            described in <xref target="ss:core_infrastructure:RaRoR" />.  Note that it is not necessary to
            retry requests over a connection with the same source network address
            or the same destination network address as the lost connection.  As
            long as the session ID, slot ID, and sequence ID in the retry match
            that of the original request, the callback target will recognize the
            request as a retry even if it did see the request prior to
            disconnect.
          </t>

          <t>
            If the connection lost is the last one associated with the
            backchannel, then the server MUST indicate that in the
            sr_status_flags field of every SEQUENCE reply until the backchannel
            is re-established.  There are two situations, each of which uses
            different status flags: no connectivity for the session's backchannel
            and no connectivity for any session backchannel of the client.  See
            <xref target="op_SEQUENCE" /> for a description of the appropriate flags in
            sr_status_flags.
          </t>
        </section>

        <section toc='exclude' anchor="ss:core_infrastructure:GCL" title="GSS Context Loss">
          <t>
            The server SHOULD monitor when the number of RPCSEC_GSS handles
            assigned to the backchannel reaches one, and when that one handle is
            near expiry (i.e., between one and two periods of lease time), and
            indicate so in the sr_status_flags field of all SEQUENCE replies.
            The server MUST indicate when all of the backchannel's assigned
            RPCSEC_GSS handles have expired via the sr_status_flags field of all
            SEQUENCE replies.
          </t>
        </section>
      </section>
    </section>

    <section toc='exclude' anchor="ss:core_infrastructure:PNaS" title="Parallel NFS and Sessions">
      <t>
        A client and server can potentially be a non-pNFS implementation, a
        metadata server implementation, a data server implementation, or two
        or three types of implementations.  The EXCHGID4_FLAG_USE_NON_PNFS,
        EXCHGID4_FLAG_USE_PNFS_MDS, and EXCHGID4_FLAG_USE_PNFS_DS flags (not
        mutually exclusive) are passed in the EXCHANGE_ID arguments and
        results to allow the client to indicate how it wants to use sessions
        created under the client ID, and to allow the server to indicate how
        it will allow the sessions to be used.  See <xref target="ss:nfsv4.1_as:CIaSC" /> for pNFS
        sessions considerations.
      </t>
    </section>
  </section>
</section>
